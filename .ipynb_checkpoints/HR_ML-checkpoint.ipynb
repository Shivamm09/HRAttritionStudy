{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "###The following columns have no information provided, and they seems to be incosistent with what \n",
    "## some of the data columns in the columns already have\n",
    "for x in data.columns:\n",
    "    if x[-4:] == \"Rate\":\n",
    "        data = data.drop(x, axis =1)\n",
    "###The following data does not provide any relevance to the data as they are either \n",
    "###Â all have the same number, or the value does not provide any information\n",
    "data = data.drop([\"EmployeeCount\", \"EmployeeNumber\", \"StandardHours\", \"Over18\", \"EducationField\", 'Education', 'JobLevel'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"Yes\": 1, \"No\":0}\n",
    "data['StockOptionLevel'] = np.where(data['StockOptionLevel'] >0, 1,0)\n",
    "data['OverTime'] = data['OverTime'].map(mapping)\n",
    "data['Gender'] = np.where(data[\"Gender\"] == \"Male\", 1,0)\n",
    "data['BusinessTravel'] = np.where(data['BusinessTravel'] == 'Travel_Frequently', 1,0)\n",
    "data['MaritalStatus'] = np.where(data['MaritalStatus'] == 'Single',0,1)\n",
    "data['TotalJobSatisfcation'] = data['EnvironmentSatisfaction'] + data['JobSatisfaction'] + data['RelationshipSatisfaction']\n",
    "data = data.drop(['EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction', 'PerformanceRating'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_to_salary = {}\n",
    "for x in data['JobRole'].unique():\n",
    "    subset = data[data['JobRole'] == x]\n",
    "    role_to_salary[x] = np.mean(subset['MonthlyIncome'])\n",
    "\n",
    "data['JobRole'] = data['JobRole'].map(role_to_salary)\n",
    "\n",
    "\n",
    "department_to_salary = {}\n",
    "for x in data['Department'].unique():\n",
    "    subset = data[data['Department'] == x]\n",
    "    department_to_salary[x] = np.mean(subset['MonthlyIncome'])\n",
    "\n",
    "data['Department'] = data['Department'].map(department_to_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical = list(data.drop(['Attrition'], axis =1).describe(include = ['O']).columns)\n",
    "numerical = list(data.describe().columns)\n",
    "x = data[numerical].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "df.columns = numerical\n",
    "# for x in categorical:\n",
    "#     df[x] = data[x]\n",
    "# df = pd.get_dummies(df, columns = categorical)\n",
    "mapping = {\"Yes\": 1, \"No\":0}\n",
    "df[\"Attrition\"] = data[\"Attrition\"].map(mapping)\n",
    "X = df.drop(\"Attrition\", axis =1)\n",
    "y = df[\"Attrition\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True, stratify = y)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42, shuffle = True, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       863\n",
      "           1       0.00      0.00      0.00       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.42      0.50      0.46      1029\n",
      "weighted avg       0.70      0.84      0.77      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       0.00      0.00      0.00        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.42      0.50      0.46       441\n",
      "weighted avg       0.70      0.84      0.77       441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, dummy_clf.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, dummy_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       863\n",
      "           1       0.88      0.40      0.55       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.89      0.69      0.74      1029\n",
      "weighted avg       0.89      0.89      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       370\n",
      "           1       0.83      0.42      0.56        71\n",
      "\n",
      "    accuracy                           0.89       441\n",
      "   macro avg       0.87      0.70      0.75       441\n",
      "weighted avg       0.89      0.89      0.88       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel=\"linear\", C =1, probability= True).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, svc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(svc.predict(X_test) ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', -1.1702826940198803),\n",
       " ('BusinessTravel', 0.6956480875197286),\n",
       " ('Department', 0.5457153870831988),\n",
       " ('DistanceFromHome', 0.674852644171164),\n",
       " ('Gender', 0.1288764888692624),\n",
       " ('JobInvolvement', -0.858109687750372),\n",
       " ('JobRole', -0.9689650948879223),\n",
       " ('MaritalStatus', -0.09619114111732685),\n",
       " ('MonthlyIncome', 0.01265262733752437),\n",
       " ('NumCompaniesWorked', 1.0273659479738066),\n",
       " ('OverTime', 1.041431318501914),\n",
       " ('PercentSalaryHike', 0.1005654949384096),\n",
       " ('StockOptionLevel', -0.7842926573758184),\n",
       " ('TotalWorkingYears', -1.2313242874890167),\n",
       " ('TrainingTimesLastYear', -0.4892138701975559),\n",
       " ('WorkLifeBalance', -0.5686634812869675),\n",
       " ('YearsAtCompany', -0.40124367618727885),\n",
       " ('YearsInCurrentRole', -1.2022445678776261),\n",
       " ('YearsSinceLastPromotion', 1.707096049398664),\n",
       " ('YearsWithCurrManager', -0.7036201180812487),\n",
       " ('TotalJobSatisfcation', -1.955425340067796)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(list(X_train.columns), svc.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       863\n",
      "           1       0.85      0.41      0.55       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.87      0.70      0.75      1029\n",
      "weighted avg       0.89      0.89      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       370\n",
      "           1       0.73      0.38      0.50        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.81      0.68      0.72       441\n",
      "weighted avg       0.87      0.88      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lr.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       863\n",
      "           1       1.00      0.01      0.01       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.92      0.50      0.46      1029\n",
      "weighted avg       0.87      0.84      0.77      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       1.00      0.03      0.05        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.92      0.51      0.48       441\n",
      "weighted avg       0.87      0.84      0.78       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lra = LogisticRegression(class_weight = {0:.85, 1:0.15}).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lra.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lra.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([439,   2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lra.predict(X_test), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([370,  71]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       863\n",
      "           1       0.94      0.99      0.96       166\n",
      "\n",
      "    accuracy                           0.99      1029\n",
      "   macro avg       0.97      0.99      0.98      1029\n",
      "weighted avg       0.99      0.99      0.99      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       370\n",
      "           1       0.43      0.44      0.43        71\n",
      "\n",
      "    accuracy                           0.82       441\n",
      "   macro avg       0.66      0.66      0.66       441\n",
      "weighted avg       0.82      0.82      0.82       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth =3, n_estimators = 100, scale_pos_weight = 4)\n",
    "clf.fit(X_train,y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, clf.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[\n",
    "    ('svc', SVC(kernel=\"linear\", C =1, probability= True, class_weight = {0:0.85, 1:0.15})),\n",
    "    ('clf', xgb.XGBClassifier(max_depth =1, n_estimators = 100, scale_pos_weight = 4)),\n",
    "    ('lr', LogisticRegression(class_weight = {0:0.85, 1:0.15}))], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       863\n",
      "           1       1.00      0.22      0.36       166\n",
      "\n",
      "    accuracy                           0.87      1029\n",
      "   macro avg       0.93      0.61      0.64      1029\n",
      "weighted avg       0.89      0.87      0.84      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93       370\n",
      "           1       0.88      0.20      0.32        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.87      0.60      0.62       441\n",
      "weighted avg       0.87      0.87      0.83       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vc.fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, vc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[\n",
    "    ('svc', SVC(kernel=\"linear\", C =1, probability= True, class_weight = 'balanced')),\n",
    "    ('clf', xgb.XGBClassifier(max_depth =1, n_estimators = 100)),\n",
    "    ('lr', LogisticRegression(class_weight = 'balanced'))], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       863\n",
      "           1       0.79      0.57      0.66       166\n",
      "\n",
      "    accuracy                           0.91      1029\n",
      "   macro avg       0.86      0.77      0.81      1029\n",
      "weighted avg       0.90      0.91      0.90      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93       370\n",
      "           1       0.65      0.46      0.54        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.77      0.71      0.73       441\n",
      "weighted avg       0.86      0.87      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vc.fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, vc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 1s 1ms/step - loss: 0.5825 - precision: 0.1630 - val_loss: 0.5197 - val_precision: 0.1786\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 175us/step - loss: 0.5067 - precision: 0.1696 - val_loss: 0.4729 - val_precision: 0.1718\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.4651 - precision: 0.1826 - val_loss: 0.4569 - val_precision: 0.1938\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.4678 - precision: 0.1976 - val_loss: 0.4299 - val_precision: 0.2068\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.4661 - precision: 0.2126 - val_loss: 0.4086 - val_precision: 0.2193\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.4532 - precision: 0.2276 - val_loss: 0.3995 - val_precision: 0.2417\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.4295 - precision: 0.2591 - val_loss: 0.3800 - val_precision: 0.2746\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.4399 - precision: 0.2793 - val_loss: 0.3769 - val_precision: 0.2893\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.4168 - precision: 0.3010 - val_loss: 0.3673 - val_precision: 0.3143\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.4079 - precision: 0.3188 - val_loss: 0.3615 - val_precision: 0.3271\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.4169 - precision: 0.3340 - val_loss: 0.3526 - val_precision: 0.3451\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.4086 - precision: 0.3524 - val_loss: 0.3463 - val_precision: 0.3613\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 143us/step - loss: 0.4062 - precision: 0.3672 - val_loss: 0.3385 - val_precision: 0.3780\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.3971 - precision: 0.3830 - val_loss: 0.3362 - val_precision: 0.3937\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.3781 - precision: 0.4004 - val_loss: 0.3325 - val_precision: 0.4086\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.3949 - precision: 0.4127 - val_loss: 0.3310 - val_precision: 0.4199\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3795 - precision: 0.4228 - val_loss: 0.3347 - val_precision: 0.4277\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.4059 - precision: 0.4308 - val_loss: 0.3327 - val_precision: 0.4348\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.3770 - precision: 0.4413 - val_loss: 0.3359 - val_precision: 0.4459\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3860 - precision: 0.4473 - val_loss: 0.3376 - val_precision: 0.4514\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.3674 - precision: 0.4520 - val_loss: 0.3282 - val_precision: 0.4554\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 149us/step - loss: 0.3731 - precision: 0.4586 - val_loss: 0.3337 - val_precision: 0.4613\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 155us/step - loss: 0.3578 - precision: 0.4638 - val_loss: 0.3320 - val_precision: 0.4680\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.3508 - precision: 0.4708 - val_loss: 0.3262 - val_precision: 0.4766\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 152us/step - loss: 0.3632 - precision: 0.4790 - val_loss: 0.3328 - val_precision: 0.4812\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 149us/step - loss: 0.3483 - precision: 0.4852 - val_loss: 0.3291 - val_precision: 0.4898\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 145us/step - loss: 0.3617 - precision: 0.4923 - val_loss: 0.3288 - val_precision: 0.4947\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3375 - precision: 0.4975 - val_loss: 0.3237 - val_precision: 0.4992\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3499 - precision: 0.5014 - val_loss: 0.3300 - val_precision: 0.5050\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3373 - precision: 0.5086 - val_loss: 0.3248 - val_precision: 0.5118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe0296bda30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0:0.85}\n",
    "dropout = 0.4\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "model.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       863\n",
      "           1       0.85      0.38      0.52       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.87      0.68      0.73      1029\n",
      "weighted avg       0.89      0.89      0.87      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       370\n",
      "           1       0.69      0.38      0.49        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.79      0.67      0.71       441\n",
      "weighted avg       0.86      0.87      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('train')\n",
    "# print(classification_report(y_train, model.predict(X_train)))\n",
    "# print('test')\n",
    "# print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in model.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 1s 1ms/step - loss: 0.3757 - precision_1: 0.1568 - val_loss: 0.3387 - val_precision_1: 0.1641\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 185us/step - loss: 0.3147 - precision_1: 0.1620 - val_loss: 0.3136 - val_precision_1: 0.1588\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 155us/step - loss: 0.3201 - precision_1: 0.1516 - val_loss: 0.3037 - val_precision_1: 0.1491\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 144us/step - loss: 0.3017 - precision_1: 0.1491 - val_loss: 0.2935 - val_precision_1: 0.1549\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.2861 - precision_1: 0.1612 - val_loss: 0.2897 - val_precision_1: 0.1704\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.2859 - precision_1: 0.1752 - val_loss: 0.2864 - val_precision_1: 0.1782\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.2640 - precision_1: 0.1826 - val_loss: 0.2829 - val_precision_1: 0.1849\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2779 - precision_1: 0.1888 - val_loss: 0.2716 - val_precision_1: 0.1917\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.2722 - precision_1: 0.1997 - val_loss: 0.2631 - val_precision_1: 0.2065\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2607 - precision_1: 0.2089 - val_loss: 0.2539 - val_precision_1: 0.2120\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.2706 - precision_1: 0.2162 - val_loss: 0.2494 - val_precision_1: 0.2204\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2656 - precision_1: 0.2230 - val_loss: 0.2400 - val_precision_1: 0.2312\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2377 - precision_1: 0.2407 - val_loss: 0.2306 - val_precision_1: 0.2536\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2572 - precision_1: 0.2587 - val_loss: 0.2278 - val_precision_1: 0.2683\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2340 - precision_1: 0.2776 - val_loss: 0.2163 - val_precision_1: 0.2855\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2483 - precision_1: 0.2941 - val_loss: 0.2128 - val_precision_1: 0.3039\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.2429 - precision_1: 0.3100 - val_loss: 0.2085 - val_precision_1: 0.3196\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2318 - precision_1: 0.3284 - val_loss: 0.2075 - val_precision_1: 0.3386\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.2430 - precision_1: 0.3444 - val_loss: 0.2069 - val_precision_1: 0.3523\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.2342 - precision_1: 0.3565 - val_loss: 0.2045 - val_precision_1: 0.3628\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2265 - precision_1: 0.3697 - val_loss: 0.2028 - val_precision_1: 0.3750\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2310 - precision_1: 0.3811 - val_loss: 0.2066 - val_precision_1: 0.3848\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2394 - precision_1: 0.3894 - val_loss: 0.2059 - val_precision_1: 0.3936\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2204 - precision_1: 0.3980 - val_loss: 0.2022 - val_precision_1: 0.4042\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2176 - precision_1: 0.4113 - val_loss: 0.1980 - val_precision_1: 0.4161\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.2172 - precision_1: 0.4203 - val_loss: 0.1976 - val_precision_1: 0.4244\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2271 - precision_1: 0.4251 - val_loss: 0.1994 - val_precision_1: 0.4283\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2218 - precision_1: 0.4311 - val_loss: 0.1972 - val_precision_1: 0.4341\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2150 - precision_1: 0.4361 - val_loss: 0.1959 - val_precision_1: 0.4400\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2112 - precision_1: 0.4436 - val_loss: 0.1974 - val_precision_1: 0.4467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe0298b4d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0:0.5, 1:0.7}\n",
    "dropout = 0.4\n",
    "modelb = keras.models.Sequential()\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modelb.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modelb.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       863\n",
      "           1       0.78      0.50      0.61       166\n",
      "\n",
      "    accuracy                           0.90      1029\n",
      "   macro avg       0.84      0.74      0.77      1029\n",
      "weighted avg       0.89      0.90      0.89      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       370\n",
      "           1       0.66      0.49      0.56        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.78      0.72      0.75       441\n",
      "weighted avg       0.87      0.88      0.87       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modelb.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modelb.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "926/926 [==============================] - 1s 1ms/step - loss: 0.5978 - precision_2: 0.1894 - val_loss: 0.5510 - val_precision_2: 0.1741\n",
      "Epoch 2/100\n",
      "926/926 [==============================] - 0s 175us/step - loss: 0.4718 - precision_2: 0.1798 - val_loss: 0.4831 - val_precision_2: 0.1887\n",
      "Epoch 3/100\n",
      "926/926 [==============================] - 0s 143us/step - loss: 0.4489 - precision_2: 0.1954 - val_loss: 0.4516 - val_precision_2: 0.1954\n",
      "Epoch 4/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.4483 - precision_2: 0.2138 - val_loss: 0.4342 - val_precision_2: 0.2239\n",
      "Epoch 5/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.4308 - precision_2: 0.2290 - val_loss: 0.4232 - val_precision_2: 0.2466\n",
      "Epoch 6/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.4194 - precision_2: 0.2556 - val_loss: 0.4083 - val_precision_2: 0.2666\n",
      "Epoch 7/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3979 - precision_2: 0.2787 - val_loss: 0.3939 - val_precision_2: 0.2937\n",
      "Epoch 8/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3867 - precision_2: 0.3065 - val_loss: 0.3864 - val_precision_2: 0.3189\n",
      "Epoch 9/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3952 - precision_2: 0.3337 - val_loss: 0.3778 - val_precision_2: 0.3437\n",
      "Epoch 10/100\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.3931 - precision_2: 0.3553 - val_loss: 0.3847 - val_precision_2: 0.3620\n",
      "Epoch 11/100\n",
      "926/926 [==============================] - 0s 161us/step - loss: 0.4016 - precision_2: 0.3708 - val_loss: 0.3769 - val_precision_2: 0.3791\n",
      "Epoch 12/100\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.3653 - precision_2: 0.3906 - val_loss: 0.3719 - val_precision_2: 0.4009\n",
      "Epoch 13/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3785 - precision_2: 0.4067 - val_loss: 0.3716 - val_precision_2: 0.4146\n",
      "Epoch 14/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3654 - precision_2: 0.4219 - val_loss: 0.3723 - val_precision_2: 0.4268\n",
      "Epoch 15/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3463 - precision_2: 0.4321 - val_loss: 0.3678 - val_precision_2: 0.4407\n",
      "Epoch 16/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3685 - precision_2: 0.4443 - val_loss: 0.3622 - val_precision_2: 0.4512\n",
      "Epoch 17/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3615 - precision_2: 0.4547 - val_loss: 0.3706 - val_precision_2: 0.4595\n",
      "Epoch 18/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3574 - precision_2: 0.4656 - val_loss: 0.3659 - val_precision_2: 0.4698\n",
      "Epoch 19/100\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.3503 - precision_2: 0.4740 - val_loss: 0.3630 - val_precision_2: 0.4801\n",
      "Epoch 20/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.3415 - precision_2: 0.4828 - val_loss: 0.3577 - val_precision_2: 0.4880\n",
      "Epoch 21/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.3479 - precision_2: 0.4906 - val_loss: 0.3520 - val_precision_2: 0.4967\n",
      "Epoch 22/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3402 - precision_2: 0.4997 - val_loss: 0.3445 - val_precision_2: 0.5053\n",
      "Epoch 23/100\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.3380 - precision_2: 0.5090 - val_loss: 0.3530 - val_precision_2: 0.5102\n",
      "Epoch 24/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.3433 - precision_2: 0.5136 - val_loss: 0.3594 - val_precision_2: 0.5170\n",
      "Epoch 25/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.3423 - precision_2: 0.5177 - val_loss: 0.3620 - val_precision_2: 0.5206\n",
      "Epoch 26/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3406 - precision_2: 0.5238 - val_loss: 0.3566 - val_precision_2: 0.5251\n",
      "Epoch 27/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3301 - precision_2: 0.5284 - val_loss: 0.3594 - val_precision_2: 0.5308\n",
      "Epoch 28/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3264 - precision_2: 0.5351 - val_loss: 0.3711 - val_precision_2: 0.5371\n",
      "Epoch 29/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3112 - precision_2: 0.5388 - val_loss: 0.3660 - val_precision_2: 0.5407\n",
      "Epoch 30/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.3343 - precision_2: 0.5432 - val_loss: 0.3541 - val_precision_2: 0.5455\n",
      "Epoch 31/100\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.3125 - precision_2: 0.5466 - val_loss: 0.3382 - val_precision_2: 0.5483\n",
      "Epoch 32/100\n",
      "926/926 [==============================] - 0s 145us/step - loss: 0.3414 - precision_2: 0.5495 - val_loss: 0.3377 - val_precision_2: 0.5500\n",
      "Epoch 33/100\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.3250 - precision_2: 0.5520 - val_loss: 0.3398 - val_precision_2: 0.5552\n",
      "Epoch 34/100\n",
      "926/926 [==============================] - 0s 155us/step - loss: 0.3378 - precision_2: 0.5561 - val_loss: 0.3377 - val_precision_2: 0.5575\n",
      "Epoch 35/100\n",
      "926/926 [==============================] - 0s 150us/step - loss: 0.3141 - precision_2: 0.5606 - val_loss: 0.3406 - val_precision_2: 0.5624\n",
      "Epoch 36/100\n",
      "926/926 [==============================] - 0s 153us/step - loss: 0.3045 - precision_2: 0.5651 - val_loss: 0.3461 - val_precision_2: 0.5678\n",
      "Epoch 37/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2975 - precision_2: 0.5701 - val_loss: 0.3468 - val_precision_2: 0.5720\n",
      "Epoch 38/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.3094 - precision_2: 0.5730 - val_loss: 0.3375 - val_precision_2: 0.5738\n",
      "Epoch 39/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.3197 - precision_2: 0.5739 - val_loss: 0.3391 - val_precision_2: 0.5748\n",
      "Epoch 40/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.3135 - precision_2: 0.5766 - val_loss: 0.3394 - val_precision_2: 0.5769\n",
      "Epoch 41/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3067 - precision_2: 0.5780 - val_loss: 0.3362 - val_precision_2: 0.5789\n",
      "Epoch 42/100\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.2948 - precision_2: 0.5804 - val_loss: 0.3452 - val_precision_2: 0.5814\n",
      "Epoch 43/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.3168 - precision_2: 0.5828 - val_loss: 0.3438 - val_precision_2: 0.5833\n",
      "Epoch 44/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.3149 - precision_2: 0.5850 - val_loss: 0.3464 - val_precision_2: 0.5859\n",
      "Epoch 45/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2973 - precision_2: 0.5872 - val_loss: 0.3491 - val_precision_2: 0.5889\n",
      "Epoch 46/100\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.3074 - precision_2: 0.5898 - val_loss: 0.3446 - val_precision_2: 0.5904\n",
      "Epoch 47/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2968 - precision_2: 0.5919 - val_loss: 0.3577 - val_precision_2: 0.5933\n",
      "Epoch 48/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2941 - precision_2: 0.5941 - val_loss: 0.3559 - val_precision_2: 0.5947\n",
      "Epoch 49/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2865 - precision_2: 0.5958 - val_loss: 0.3517 - val_precision_2: 0.5969\n",
      "Epoch 50/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.3167 - precision_2: 0.5968 - val_loss: 0.3561 - val_precision_2: 0.5970\n",
      "Epoch 51/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.3077 - precision_2: 0.5976 - val_loss: 0.3428 - val_precision_2: 0.5984\n",
      "Epoch 52/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.3000 - precision_2: 0.5999 - val_loss: 0.3521 - val_precision_2: 0.6007\n",
      "Epoch 53/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2852 - precision_2: 0.6019 - val_loss: 0.3497 - val_precision_2: 0.6031\n",
      "Epoch 54/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2794 - precision_2: 0.6038 - val_loss: 0.3529 - val_precision_2: 0.6044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.2889 - precision_2: 0.6059 - val_loss: 0.3454 - val_precision_2: 0.6055\n",
      "Epoch 56/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2851 - precision_2: 0.6070 - val_loss: 0.3521 - val_precision_2: 0.6083\n",
      "Epoch 57/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2745 - precision_2: 0.6098 - val_loss: 0.3586 - val_precision_2: 0.6107\n",
      "Epoch 58/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.3037 - precision_2: 0.6105 - val_loss: 0.3628 - val_precision_2: 0.6108\n",
      "Epoch 59/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3001 - precision_2: 0.6112 - val_loss: 0.3482 - val_precision_2: 0.6119\n",
      "Epoch 60/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2927 - precision_2: 0.6134 - val_loss: 0.3503 - val_precision_2: 0.6138\n",
      "Epoch 61/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2846 - precision_2: 0.6144 - val_loss: 0.3632 - val_precision_2: 0.6151\n",
      "Epoch 62/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2814 - precision_2: 0.6158 - val_loss: 0.3706 - val_precision_2: 0.6164\n",
      "Epoch 63/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2892 - precision_2: 0.6169 - val_loss: 0.3861 - val_precision_2: 0.6175\n",
      "Epoch 64/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2881 - precision_2: 0.6182 - val_loss: 0.3639 - val_precision_2: 0.6189\n",
      "Epoch 65/100\n",
      "926/926 [==============================] - 0s 126us/step - loss: 0.2625 - precision_2: 0.6200 - val_loss: 0.3598 - val_precision_2: 0.6217\n",
      "Epoch 66/100\n",
      "926/926 [==============================] - 0s 126us/step - loss: 0.2963 - precision_2: 0.6222 - val_loss: 0.3736 - val_precision_2: 0.6224\n",
      "Epoch 67/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2805 - precision_2: 0.6227 - val_loss: 0.3814 - val_precision_2: 0.6232\n",
      "Epoch 68/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2886 - precision_2: 0.6241 - val_loss: 0.3867 - val_precision_2: 0.6243\n",
      "Epoch 69/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2742 - precision_2: 0.6252 - val_loss: 0.3866 - val_precision_2: 0.6258\n",
      "Epoch 70/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2884 - precision_2: 0.6263 - val_loss: 0.3722 - val_precision_2: 0.6271\n",
      "Epoch 71/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.2778 - precision_2: 0.6281 - val_loss: 0.3625 - val_precision_2: 0.6287\n",
      "Epoch 72/100\n",
      "926/926 [==============================] - 0s 125us/step - loss: 0.2768 - precision_2: 0.6290 - val_loss: 0.3722 - val_precision_2: 0.6295\n",
      "Epoch 73/100\n",
      "926/926 [==============================] - 0s 126us/step - loss: 0.2675 - precision_2: 0.6304 - val_loss: 0.3692 - val_precision_2: 0.6305\n",
      "Epoch 74/100\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.2759 - precision_2: 0.6312 - val_loss: 0.3744 - val_precision_2: 0.6315\n",
      "Epoch 75/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2640 - precision_2: 0.6322 - val_loss: 0.3622 - val_precision_2: 0.6328\n",
      "Epoch 76/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.3022 - precision_2: 0.6327 - val_loss: 0.3550 - val_precision_2: 0.6333\n",
      "Epoch 77/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2739 - precision_2: 0.6335 - val_loss: 0.3578 - val_precision_2: 0.6341\n",
      "Epoch 78/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.2640 - precision_2: 0.6346 - val_loss: 0.3692 - val_precision_2: 0.6352\n",
      "Epoch 79/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2850 - precision_2: 0.6356 - val_loss: 0.3703 - val_precision_2: 0.6363\n",
      "Epoch 80/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2792 - precision_2: 0.6359 - val_loss: 0.3859 - val_precision_2: 0.6360\n",
      "Epoch 81/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2703 - precision_2: 0.6369 - val_loss: 0.3939 - val_precision_2: 0.6372\n",
      "Epoch 82/100\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.2774 - precision_2: 0.6379 - val_loss: 0.3949 - val_precision_2: 0.6383\n",
      "Epoch 83/100\n",
      "926/926 [==============================] - 0s 157us/step - loss: 0.2792 - precision_2: 0.6385 - val_loss: 0.3747 - val_precision_2: 0.6387\n",
      "Epoch 84/100\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.2506 - precision_2: 0.6393 - val_loss: 0.3925 - val_precision_2: 0.6397\n",
      "Epoch 85/100\n",
      "926/926 [==============================] - 0s 149us/step - loss: 0.2535 - precision_2: 0.6401 - val_loss: 0.4099 - val_precision_2: 0.6406\n",
      "Epoch 86/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2633 - precision_2: 0.6410 - val_loss: 0.4254 - val_precision_2: 0.6417\n",
      "Epoch 87/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2564 - precision_2: 0.6424 - val_loss: 0.4061 - val_precision_2: 0.6426\n",
      "Epoch 88/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2919 - precision_2: 0.6428 - val_loss: 0.3941 - val_precision_2: 0.6428\n",
      "Epoch 89/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2582 - precision_2: 0.6437 - val_loss: 0.4072 - val_precision_2: 0.6441\n",
      "Epoch 90/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2590 - precision_2: 0.6447 - val_loss: 0.4050 - val_precision_2: 0.6450\n",
      "Epoch 91/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2619 - precision_2: 0.6455 - val_loss: 0.3945 - val_precision_2: 0.6464\n",
      "Epoch 92/100\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.2795 - precision_2: 0.6466 - val_loss: 0.3862 - val_precision_2: 0.6464\n",
      "Epoch 93/100\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.2430 - precision_2: 0.6473 - val_loss: 0.3931 - val_precision_2: 0.6476\n",
      "Epoch 94/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.2678 - precision_2: 0.6481 - val_loss: 0.4037 - val_precision_2: 0.6487\n",
      "Epoch 95/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2630 - precision_2: 0.6494 - val_loss: 0.3962 - val_precision_2: 0.6498\n",
      "Epoch 96/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.2618 - precision_2: 0.6502 - val_loss: 0.3918 - val_precision_2: 0.6499\n",
      "Epoch 97/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2522 - precision_2: 0.6503 - val_loss: 0.3973 - val_precision_2: 0.6509\n",
      "Epoch 98/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2276 - precision_2: 0.6519 - val_loss: 0.4127 - val_precision_2: 0.6521\n",
      "Epoch 99/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2639 - precision_2: 0.6524 - val_loss: 0.4168 - val_precision_2: 0.6527\n",
      "Epoch 100/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2523 - precision_2: 0.6534 - val_loss: 0.4509 - val_precision_2: 0.6539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe029b9f7f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = 0.3\n",
    "modelc = keras.models.Sequential()\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modelc.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modelc.fit(X_train, y_train, epochs = 100, batch_size = 20, validation_split=0.1, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       863\n",
      "           1       0.92      0.55      0.69       166\n",
      "\n",
      "    accuracy                           0.92      1029\n",
      "   macro avg       0.92      0.77      0.82      1029\n",
      "weighted avg       0.92      0.92      0.91      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       370\n",
      "           1       0.78      0.35      0.49        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.83      0.67      0.71       441\n",
      "weighted avg       0.87      0.88      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modelc.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modelc.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 0s 402us/step - loss: 0.6870 - precision_3: 0.1645 - val_loss: 0.4989 - val_precision_3: 0.1933\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 88us/step - loss: 0.4587 - precision_3: 0.1952 - val_loss: 0.4765 - val_precision_3: 0.2000\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 69us/step - loss: 0.3885 - precision_3: 0.2126 - val_loss: 0.4530 - val_precision_3: 0.2271\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 77us/step - loss: 0.3932 - precision_3: 0.2435 - val_loss: 0.4308 - val_precision_3: 0.2571\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 67us/step - loss: 0.3854 - precision_3: 0.2774 - val_loss: 0.4152 - val_precision_3: 0.2874\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.3606 - precision_3: 0.3062 - val_loss: 0.4071 - val_precision_3: 0.3183\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.3677 - precision_3: 0.3296 - val_loss: 0.3848 - val_precision_3: 0.3404\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 64us/step - loss: 0.3450 - precision_3: 0.3552 - val_loss: 0.3800 - val_precision_3: 0.3706\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 64us/step - loss: 0.3477 - precision_3: 0.3810 - val_loss: 0.3674 - val_precision_3: 0.3928\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 66us/step - loss: 0.3310 - precision_3: 0.4050 - val_loss: 0.3633 - val_precision_3: 0.4135\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3413 - precision_3: 0.4213 - val_loss: 0.3568 - val_precision_3: 0.4300\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.3407 - precision_3: 0.4365 - val_loss: 0.3540 - val_precision_3: 0.4453\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3429 - precision_3: 0.4531 - val_loss: 0.3539 - val_precision_3: 0.4581\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 61us/step - loss: 0.3002 - precision_3: 0.4662 - val_loss: 0.3556 - val_precision_3: 0.4765\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3183 - precision_3: 0.4834 - val_loss: 0.3532 - val_precision_3: 0.4910\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.3163 - precision_3: 0.4997 - val_loss: 0.3516 - val_precision_3: 0.5061\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.3402 - precision_3: 0.5120 - val_loss: 0.3436 - val_precision_3: 0.5137\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.3195 - precision_3: 0.5198 - val_loss: 0.3405 - val_precision_3: 0.5257\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3081 - precision_3: 0.5316 - val_loss: 0.3429 - val_precision_3: 0.5366\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 73us/step - loss: 0.3171 - precision_3: 0.5413 - val_loss: 0.3454 - val_precision_3: 0.5421\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 72us/step - loss: 0.3242 - precision_3: 0.5443 - val_loss: 0.3450 - val_precision_3: 0.5498\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 71us/step - loss: 0.3122 - precision_3: 0.5525 - val_loss: 0.3478 - val_precision_3: 0.5539\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 70us/step - loss: 0.3235 - precision_3: 0.5570 - val_loss: 0.3469 - val_precision_3: 0.5596\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 67us/step - loss: 0.3163 - precision_3: 0.5623 - val_loss: 0.3460 - val_precision_3: 0.5659\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 75us/step - loss: 0.2993 - precision_3: 0.5694 - val_loss: 0.3407 - val_precision_3: 0.5721\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.2967 - precision_3: 0.5748 - val_loss: 0.3430 - val_precision_3: 0.5778\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 71us/step - loss: 0.3134 - precision_3: 0.5813 - val_loss: 0.3411 - val_precision_3: 0.5830\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 70us/step - loss: 0.3133 - precision_3: 0.5858 - val_loss: 0.3439 - val_precision_3: 0.5893\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 70us/step - loss: 0.3009 - precision_3: 0.5923 - val_loss: 0.3392 - val_precision_3: 0.5951\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 75us/step - loss: 0.2927 - precision_3: 0.5991 - val_loss: 0.3403 - val_precision_3: 0.6016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdfda515520>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "modeld = keras.models.Sequential()\n",
    "modeld.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modeld.add(keras.layers.BatchNormalization())\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Activation('relu'))\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modeld.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modeld.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       863\n",
      "           1       0.89      0.40      0.56       166\n",
      "\n",
      "    accuracy                           0.90      1029\n",
      "   macro avg       0.89      0.70      0.75      1029\n",
      "weighted avg       0.90      0.90      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       370\n",
      "           1       0.77      0.38      0.51        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.83      0.68      0.72       441\n",
      "weighted avg       0.87      0.88      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modeld.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modeld.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 0s 437us/step - loss: 0.2285 - precision_4: 0.1715 - val_loss: 0.1971 - val_precision_4: 0.2162\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 79us/step - loss: 0.1852 - precision_4: 0.2076 - val_loss: 0.1852 - val_precision_4: 0.1951\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 70us/step - loss: 0.1705 - precision_4: 0.1951 - val_loss: 0.1750 - val_precision_4: 0.1951\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 78us/step - loss: 0.1574 - precision_4: 0.2245 - val_loss: 0.1673 - val_precision_4: 0.2500\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1528 - precision_4: 0.2732 - val_loss: 0.1599 - val_precision_4: 0.3200\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 73us/step - loss: 0.1460 - precision_4: 0.3546 - val_loss: 0.1532 - val_precision_4: 0.3750\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 72us/step - loss: 0.1471 - precision_4: 0.4081 - val_loss: 0.1491 - val_precision_4: 0.4286\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 79us/step - loss: 0.1458 - precision_4: 0.4476 - val_loss: 0.1441 - val_precision_4: 0.4765\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 72us/step - loss: 0.1499 - precision_4: 0.4923 - val_loss: 0.1431 - val_precision_4: 0.5268\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 75us/step - loss: 0.1444 - precision_4: 0.5397 - val_loss: 0.1417 - val_precision_4: 0.5627\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 76us/step - loss: 0.1427 - precision_4: 0.5759 - val_loss: 0.1407 - val_precision_4: 0.5916\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 75us/step - loss: 0.1391 - precision_4: 0.6077 - val_loss: 0.1401 - val_precision_4: 0.6220\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1330 - precision_4: 0.6264 - val_loss: 0.1403 - val_precision_4: 0.6267\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1356 - precision_4: 0.6386 - val_loss: 0.1397 - val_precision_4: 0.6496\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 75us/step - loss: 0.1389 - precision_4: 0.6669 - val_loss: 0.1397 - val_precision_4: 0.6768\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 75us/step - loss: 0.1329 - precision_4: 0.6884 - val_loss: 0.1392 - val_precision_4: 0.6990\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 72us/step - loss: 0.1350 - precision_4: 0.7074 - val_loss: 0.1394 - val_precision_4: 0.7187\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 69us/step - loss: 0.1336 - precision_4: 0.7273 - val_loss: 0.1390 - val_precision_4: 0.7321\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 68us/step - loss: 0.1294 - precision_4: 0.7378 - val_loss: 0.1384 - val_precision_4: 0.7403\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 68us/step - loss: 0.1280 - precision_4: 0.7488 - val_loss: 0.1378 - val_precision_4: 0.7517\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 66us/step - loss: 0.1260 - precision_4: 0.7582 - val_loss: 0.1383 - val_precision_4: 0.7632\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.1282 - precision_4: 0.7671 - val_loss: 0.1367 - val_precision_4: 0.7710\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 64us/step - loss: 0.1231 - precision_4: 0.7758 - val_loss: 0.1360 - val_precision_4: 0.7810\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 64us/step - loss: 0.1333 - precision_4: 0.7825 - val_loss: 0.1364 - val_precision_4: 0.7846\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 70us/step - loss: 0.1277 - precision_4: 0.7897 - val_loss: 0.1367 - val_precision_4: 0.7944\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 72us/step - loss: 0.1318 - precision_4: 0.7985 - val_loss: 0.1381 - val_precision_4: 0.8014\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 71us/step - loss: 0.1272 - precision_4: 0.8023 - val_loss: 0.1371 - val_precision_4: 0.8033\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 72us/step - loss: 0.1234 - precision_4: 0.8035 - val_loss: 0.1386 - val_precision_4: 0.8063\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 64us/step - loss: 0.1267 - precision_4: 0.8080 - val_loss: 0.1397 - val_precision_4: 0.8083\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 58us/step - loss: 0.1249 - precision_4: 0.8110 - val_loss: 0.1382 - val_precision_4: 0.8117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdfebd8b5e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight= {0:0.7, 1:0.3}\n",
    "dropout = 0.2\n",
    "modeld = keras.models.Sequential()\n",
    "modeld.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.BatchNormalization())\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Activation('relu'))\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modeld.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modeld.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       863\n",
      "           1       0.96      0.30      0.46       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.92      0.65      0.70      1029\n",
      "weighted avg       0.89      0.89      0.86      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93       370\n",
      "           1       0.84      0.30      0.44        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.86      0.64      0.68       441\n",
      "weighted avg       0.87      0.88      0.85       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modeld.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modeld.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###For Deep Learning we are going to be using modeld\n",
    "###For machine Learning use svc\n",
    "###VC for very high precision and acceptable recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Example of how to use Machine Learning Model - will talk about SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mutiplication = list(X_test.iloc[0])*svc.coef_[0]\n",
    "# mutiplication_abs = np.abs(mutiplication)\n",
    "indexes = np.where(mutiplication <= -0.3)\n",
    "plt.barh(X_train.columns,mutiplication, color = '#FFa5a5', linewidth = 1, edgecolor = 'black')\n",
    "plt.barh(X_train.columns[indexes],mutiplication[indexes], color = '#9fadfa', linewidth = 1, edgecolor = 'black')\n",
    "plt.legend(['Affecting', ' Not Affecting'])\n",
    "plt.title('Factors Affecting the Attrition Problem for this Employee')\n",
    "plt.savefig('Factors.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', -1.1702826940198803),\n",
       " ('BusinessTravel', 0.6956480875197286),\n",
       " ('Department', 0.5457153870831988),\n",
       " ('DistanceFromHome', 0.674852644171164),\n",
       " ('Gender', 0.1288764888692624),\n",
       " ('JobInvolvement', -0.858109687750372),\n",
       " ('JobRole', -0.9689650948879223),\n",
       " ('MaritalStatus', -0.09619114111732685),\n",
       " ('MonthlyIncome', 0.01265262733752437),\n",
       " ('NumCompaniesWorked', 1.0273659479738066),\n",
       " ('OverTime', 1.041431318501914),\n",
       " ('PercentSalaryHike', 0.1005654949384096),\n",
       " ('StockOptionLevel', -0.7842926573758184),\n",
       " ('TotalWorkingYears', -1.2313242874890167),\n",
       " ('TrainingTimesLastYear', -0.4892138701975559),\n",
       " ('WorkLifeBalance', -0.5686634812869675),\n",
       " ('YearsAtCompany', -0.40124367618727885),\n",
       " ('YearsInCurrentRole', -1.2022445678776261),\n",
       " ('YearsSinceLastPromotion', 1.707096049398664),\n",
       " ('YearsWithCurrManager', -0.7036201180812487),\n",
       " ('TotalJobSatisfcation', -1.955425340067796)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X_test.columns,svc.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
