{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "###The following columns have no information provided, and they seems to be incosistent with what \n",
    "## some of the data columns in the columns already have\n",
    "for x in data.columns:\n",
    "    if x[-4:] == \"Rate\":\n",
    "        data = data.drop(x, axis =1)\n",
    "###The following data does not provide any relevance to the data as they are either \n",
    "###Â all have the same number, or the value does not provide any information\n",
    "data = data.drop([\"EmployeeCount\", \"EmployeeNumber\", \"StandardHours\", \"Over18\"], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns= data.describe(include = [np.number]).columns\n",
    "categorical_columns = data.describe(include = ['O']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = categorical_columns.drop('Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[numerical_columns].values\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "df.columns = numerical_columns\n",
    "for x in categorical_columns:\n",
    "    df[x] = data[x]\n",
    "df = pd.get_dummies(df, columns = categorical_columns)\n",
    "mapping = {\"Yes\": 1, \"No\":0}\n",
    "df[\"Attrition\"] = data[\"Attrition\"].map(mapping)\n",
    "X = df.drop(\"Attrition\", axis =1)\n",
    "y = df[\"Attrition\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       863\n",
      "           1       0.85      0.45      0.59       166\n",
      "\n",
      "    accuracy                           0.90      1029\n",
      "   macro avg       0.88      0.72      0.77      1029\n",
      "weighted avg       0.90      0.90      0.89      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.93       370\n",
      "           1       0.76      0.41      0.53        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.83      0.69      0.73       441\n",
      "weighted avg       0.87      0.88      0.87       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel=\"linear\", C =1).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, svc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       863\n",
      "           1       0.82      0.43      0.57       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.86      0.71      0.75      1029\n",
      "weighted avg       0.89      0.89      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93       370\n",
      "           1       0.72      0.41      0.52        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.81      0.69      0.73       441\n",
      "weighted avg       0.87      0.88      0.87       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lr.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       863\n",
      "           1       1.00      0.02      0.05       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.92      0.51      0.48      1029\n",
      "weighted avg       0.87      0.84      0.77      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       1.00      0.03      0.05        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.92      0.51      0.48       441\n",
      "weighted avg       0.87      0.84      0.78       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0:0.85, 1:0.15}\n",
    "lr = LogisticRegression(class_weight= class_weight).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lr.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.28      0.44       863\n",
      "           1       0.21      1.00      0.35       166\n",
      "\n",
      "    accuracy                           0.40      1029\n",
      "   macro avg       0.61      0.64      0.39      1029\n",
      "weighted avg       0.87      0.40      0.42      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.25      0.40       370\n",
      "           1       0.20      0.96      0.33        71\n",
      "\n",
      "    accuracy                           0.36       441\n",
      "   macro avg       0.58      0.60      0.36       441\n",
      "weighted avg       0.84      0.36      0.38       441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0:1, 1:100}\n",
    "lr = LogisticRegression(class_weight= class_weight).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lr.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.30      0.46       863\n",
      "           1       0.22      1.00      0.35       166\n",
      "\n",
      "    accuracy                           0.41      1029\n",
      "   macro avg       0.61      0.65      0.41      1029\n",
      "weighted avg       0.87      0.41      0.44      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.31      0.47       370\n",
      "           1       0.21      0.94      0.34        71\n",
      "\n",
      "    accuracy                           0.41       441\n",
      "   macro avg       0.59      0.63      0.41       441\n",
      "weighted avg       0.84      0.41      0.45       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0:1, 1:100}\n",
    "svc = SVC(kernel=\"linear\", C =1, class_weight= class_weight).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, svc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>...</th>\n",
       "      <th>PerformanceRating</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel              Department  DistanceFromHome  \\\n",
       "0   41       Yes      Travel_Rarely                   Sales                 1   \n",
       "1   49        No  Travel_Frequently  Research & Development                 8   \n",
       "2   37       Yes      Travel_Rarely  Research & Development                 2   \n",
       "3   33        No  Travel_Frequently  Research & Development                 3   \n",
       "4   27        No      Travel_Rarely  Research & Development                 2   \n",
       "\n",
       "   Education EducationField  EnvironmentSatisfaction  Gender  JobInvolvement  \\\n",
       "0          2  Life Sciences                        2  Female               3   \n",
       "1          1  Life Sciences                        3    Male               2   \n",
       "2          2          Other                        4    Male               2   \n",
       "3          4  Life Sciences                        4  Female               3   \n",
       "4          1        Medical                        1    Male               3   \n",
       "\n",
       "   ...  PerformanceRating RelationshipSatisfaction  StockOptionLevel  \\\n",
       "0  ...                  3                        1                 0   \n",
       "1  ...                  4                        4                 1   \n",
       "2  ...                  3                        2                 0   \n",
       "3  ...                  3                        3                 0   \n",
       "4  ...                  3                        4                 1   \n",
       "\n",
       "  TotalWorkingYears  TrainingTimesLastYear  WorkLifeBalance YearsAtCompany  \\\n",
       "0                 8                      0                1              6   \n",
       "1                10                      3                3             10   \n",
       "2                 7                      3                3              0   \n",
       "3                 8                      3                3              8   \n",
       "4                 6                      3                3              2   \n",
       "\n",
       "   YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                   4                        0                     5  \n",
       "1                   7                        1                     7  \n",
       "2                   0                        0                     0  \n",
       "3                   7                        3                     0  \n",
       "4                   2                        2                     2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalJobSatisfaction'] = data['EnvironmentSatisfaction'] + data['JobSatisfaction'] + data['RelationshipSatisfaction']\n",
    "data['MarriedAndBad_Worklife_Balance'] = np.where(data['MaritalStatus']=='Married', \n",
    "                                               data['WorkLifeBalance']-2,\n",
    "                                               data['WorkLifeBalance']+1)\n",
    "data['DistanceFromHome_rootedTo_JobSatisfaction'] = data['DistanceFromHome']**(1/data['JobSatisfaction'])\n",
    "values = ['Married', 'Divorced']\n",
    "data['Mothers'] = np.where((data['Gender']=='Female') & (data['Age']>=36) & ((data['MaritalStatus'].isin(values))), 1,0)\n",
    "data['OldLowEmployeeTendToStay'] = data['YearsAtCompany'] / data['JobLevel']\n",
    "data['JobInvolment_On_Salary']= data['JobInvolvement'] / data['MonthlyIncome'] * 1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns= data.describe(include = [np.number]).columns\n",
    "categorical_columns = data.describe(include = ['O']).columns\n",
    "categorical_columns = categorical_columns.drop('Attrition')\n",
    "x = data[numerical_columns].values\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "df.columns = numerical_columns\n",
    "for x in categorical_columns:\n",
    "    df[x] = data[x]\n",
    "df = pd.get_dummies(df, columns = categorical_columns)\n",
    "mapping = {\"Yes\": 1, \"No\":0}\n",
    "df[\"Attrition\"] = data[\"Attrition\"].map(mapping)\n",
    "X = df.drop(\"Attrition\", axis =1)\n",
    "y = df[\"Attrition\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.92       863\n",
      "           1       1.00      0.04      0.07       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.92      0.52      0.49      1029\n",
      "weighted avg       0.87      0.84      0.78      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       1.00      0.03      0.05        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.92      0.51      0.48       441\n",
      "weighted avg       0.87      0.84      0.78       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0:0.85, 1:0.15}\n",
    "lr = LogisticRegression(class_weight= class_weight).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lr.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "926/926 [==============================] - 1s 814us/step - loss: 0.2790 - recall_1: 0.1089 - val_loss: 0.3835 - val_recall_1: 0.0380\n",
      "Epoch 2/100\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.1394 - recall_1: 0.0256 - val_loss: 0.2964 - val_recall_1: 0.0185\n",
      "Epoch 3/100\n",
      "926/926 [==============================] - 0s 113us/step - loss: 0.1249 - recall_1: 0.0150 - val_loss: 0.2561 - val_recall_1: 0.0122\n",
      "Epoch 4/100\n",
      "926/926 [==============================] - 0s 113us/step - loss: 0.1188 - recall_1: 0.0104 - val_loss: 0.2382 - val_recall_1: 0.0091\n",
      "Epoch 5/100\n",
      "926/926 [==============================] - 0s 116us/step - loss: 0.1146 - recall_1: 0.0081 - val_loss: 0.2234 - val_recall_1: 0.0073\n",
      "Epoch 6/100\n",
      "926/926 [==============================] - 0s 110us/step - loss: 0.1125 - recall_1: 0.0066 - val_loss: 0.1990 - val_recall_1: 0.0061\n",
      "Epoch 7/100\n",
      "926/926 [==============================] - 0s 106us/step - loss: 0.1124 - recall_1: 0.0056 - val_loss: 0.1887 - val_recall_1: 0.0052\n",
      "Epoch 8/100\n",
      "926/926 [==============================] - 0s 113us/step - loss: 0.1090 - recall_1: 0.0049 - val_loss: 0.1800 - val_recall_1: 0.0045\n",
      "Epoch 9/100\n",
      "926/926 [==============================] - 0s 108us/step - loss: 0.1071 - recall_1: 0.0043 - val_loss: 0.1686 - val_recall_1: 0.0040\n",
      "Epoch 10/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.1066 - recall_1: 0.0038 - val_loss: 0.1611 - val_recall_1: 0.0036\n",
      "Epoch 11/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.1046 - recall_1: 0.0035 - val_loss: 0.1807 - val_recall_1: 0.0033\n",
      "Epoch 12/100\n",
      "926/926 [==============================] - 0s 117us/step - loss: 0.1039 - recall_1: 0.0032 - val_loss: 0.1925 - val_recall_1: 0.0030\n",
      "Epoch 13/100\n",
      "926/926 [==============================] - 0s 109us/step - loss: 0.1049 - recall_1: 0.0029 - val_loss: 0.1895 - val_recall_1: 0.0028\n",
      "Epoch 14/100\n",
      "926/926 [==============================] - 0s 115us/step - loss: 0.0994 - recall_1: 0.0027 - val_loss: 0.1928 - val_recall_1: 0.0026\n",
      "Epoch 15/100\n",
      "926/926 [==============================] - 0s 119us/step - loss: 0.1001 - recall_1: 0.0029 - val_loss: 0.1883 - val_recall_1: 0.0032\n",
      "Epoch 16/100\n",
      "926/926 [==============================] - 0s 103us/step - loss: 0.0984 - recall_1: 0.0031 - val_loss: 0.1931 - val_recall_1: 0.0040\n",
      "Epoch 17/100\n",
      "926/926 [==============================] - 0s 108us/step - loss: 0.0959 - recall_1: 0.0044 - val_loss: 0.2065 - val_recall_1: 0.0057\n",
      "Epoch 18/100\n",
      "926/926 [==============================] - 0s 102us/step - loss: 0.0958 - recall_1: 0.0063 - val_loss: 0.1968 - val_recall_1: 0.0074\n",
      "Epoch 19/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0976 - recall_1: 0.0086 - val_loss: 0.2135 - val_recall_1: 0.0105\n",
      "Epoch 20/100\n",
      "926/926 [==============================] - 0s 108us/step - loss: 0.0960 - recall_1: 0.0110 - val_loss: 0.2153 - val_recall_1: 0.0131\n",
      "Epoch 21/100\n",
      "926/926 [==============================] - 0s 101us/step - loss: 0.0981 - recall_1: 0.0142 - val_loss: 0.2116 - val_recall_1: 0.0162\n",
      "Epoch 22/100\n",
      "926/926 [==============================] - 0s 97us/step - loss: 0.0952 - recall_1: 0.0182 - val_loss: 0.2087 - val_recall_1: 0.0209\n",
      "Epoch 23/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.0896 - recall_1: 0.0227 - val_loss: 0.2350 - val_recall_1: 0.0253\n",
      "Epoch 24/100\n",
      "926/926 [==============================] - 0s 101us/step - loss: 0.0909 - recall_1: 0.0275 - val_loss: 0.2036 - val_recall_1: 0.0298\n",
      "Epoch 25/100\n",
      "926/926 [==============================] - 0s 100us/step - loss: 0.0935 - recall_1: 0.0316 - val_loss: 0.2340 - val_recall_1: 0.0342\n",
      "Epoch 26/100\n",
      "926/926 [==============================] - 0s 105us/step - loss: 0.0894 - recall_1: 0.0371 - val_loss: 0.2091 - val_recall_1: 0.0402\n",
      "Epoch 27/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0928 - recall_1: 0.0419 - val_loss: 0.2265 - val_recall_1: 0.0448\n",
      "Epoch 28/100\n",
      "926/926 [==============================] - 0s 97us/step - loss: 0.0865 - recall_1: 0.0477 - val_loss: 0.2049 - val_recall_1: 0.0505\n",
      "Epoch 29/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0903 - recall_1: 0.0522 - val_loss: 0.2493 - val_recall_1: 0.0551\n",
      "Epoch 30/100\n",
      "926/926 [==============================] - 0s 98us/step - loss: 0.0874 - recall_1: 0.0567 - val_loss: 0.2492 - val_recall_1: 0.0600\n",
      "Epoch 31/100\n",
      "926/926 [==============================] - 0s 106us/step - loss: 0.0878 - recall_1: 0.0616 - val_loss: 0.2095 - val_recall_1: 0.0632\n",
      "Epoch 32/100\n",
      "926/926 [==============================] - 0s 118us/step - loss: 0.0881 - recall_1: 0.0646 - val_loss: 0.2024 - val_recall_1: 0.0665\n",
      "Epoch 33/100\n",
      "926/926 [==============================] - 0s 108us/step - loss: 0.0880 - recall_1: 0.0673 - val_loss: 0.2135 - val_recall_1: 0.0692\n",
      "Epoch 34/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.0833 - recall_1: 0.0716 - val_loss: 0.2278 - val_recall_1: 0.0751\n",
      "Epoch 35/100\n",
      "926/926 [==============================] - 0s 102us/step - loss: 0.0853 - recall_1: 0.0778 - val_loss: 0.2212 - val_recall_1: 0.0803\n",
      "Epoch 36/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0877 - recall_1: 0.0820 - val_loss: 0.2010 - val_recall_1: 0.0847\n",
      "Epoch 37/100\n",
      "926/926 [==============================] - 0s 96us/step - loss: 0.0929 - recall_1: 0.0861 - val_loss: 0.2045 - val_recall_1: 0.0872\n",
      "Epoch 38/100\n",
      "926/926 [==============================] - 0s 103us/step - loss: 0.0903 - recall_1: 0.0880 - val_loss: 0.2208 - val_recall_1: 0.0881\n",
      "Epoch 39/100\n",
      "926/926 [==============================] - 0s 102us/step - loss: 0.0760 - recall_1: 0.0904 - val_loss: 0.2197 - val_recall_1: 0.0934\n",
      "Epoch 40/100\n",
      "926/926 [==============================] - 0s 97us/step - loss: 0.0803 - recall_1: 0.0948 - val_loss: 0.1973 - val_recall_1: 0.0959\n",
      "Epoch 41/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0764 - recall_1: 0.0981 - val_loss: 0.2271 - val_recall_1: 0.1011\n",
      "Epoch 42/100\n",
      "926/926 [==============================] - 0s 103us/step - loss: 0.0817 - recall_1: 0.1043 - val_loss: 0.2004 - val_recall_1: 0.1071\n",
      "Epoch 43/100\n",
      "926/926 [==============================] - 0s 112us/step - loss: 0.0748 - recall_1: 0.1089 - val_loss: 0.2203 - val_recall_1: 0.1119\n",
      "Epoch 44/100\n",
      "926/926 [==============================] - 0s 113us/step - loss: 0.0782 - recall_1: 0.1150 - val_loss: 0.2275 - val_recall_1: 0.1179\n",
      "Epoch 45/100\n",
      "926/926 [==============================] - 0s 116us/step - loss: 0.0800 - recall_1: 0.1198 - val_loss: 0.2255 - val_recall_1: 0.1226\n",
      "Epoch 46/100\n",
      "926/926 [==============================] - 0s 117us/step - loss: 0.0778 - recall_1: 0.1248 - val_loss: 0.2221 - val_recall_1: 0.1270\n",
      "Epoch 47/100\n",
      "926/926 [==============================] - 0s 111us/step - loss: 0.0770 - recall_1: 0.1296 - val_loss: 0.1923 - val_recall_1: 0.1324\n",
      "Epoch 48/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0803 - recall_1: 0.1345 - val_loss: 0.1798 - val_recall_1: 0.1365\n",
      "Epoch 49/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0754 - recall_1: 0.1380 - val_loss: 0.2238 - val_recall_1: 0.1407\n",
      "Epoch 50/100\n",
      "926/926 [==============================] - 0s 97us/step - loss: 0.0741 - recall_1: 0.1433 - val_loss: 0.2172 - val_recall_1: 0.1464\n",
      "Epoch 51/100\n",
      "926/926 [==============================] - 0s 112us/step - loss: 0.0784 - recall_1: 0.1486 - val_loss: 0.2027 - val_recall_1: 0.1509\n",
      "Epoch 52/100\n",
      "926/926 [==============================] - 0s 116us/step - loss: 0.0730 - recall_1: 0.1530 - val_loss: 0.1920 - val_recall_1: 0.1557\n",
      "Epoch 53/100\n",
      "926/926 [==============================] - 0s 113us/step - loss: 0.0841 - recall_1: 0.1569 - val_loss: 0.1813 - val_recall_1: 0.1586\n",
      "Epoch 54/100\n",
      "926/926 [==============================] - 0s 117us/step - loss: 0.0756 - recall_1: 0.1604 - val_loss: 0.2300 - val_recall_1: 0.1623\n",
      "Epoch 55/100\n",
      "926/926 [==============================] - 0s 114us/step - loss: 0.0798 - recall_1: 0.1642 - val_loss: 0.2479 - val_recall_1: 0.1660\n",
      "Epoch 56/100\n",
      "926/926 [==============================] - 0s 111us/step - loss: 0.0757 - recall_1: 0.1682 - val_loss: 0.2640 - val_recall_1: 0.1705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "926/926 [==============================] - 0s 116us/step - loss: 0.0778 - recall_1: 0.1722 - val_loss: 0.2370 - val_recall_1: 0.1739\n",
      "Epoch 58/100\n",
      "926/926 [==============================] - 0s 114us/step - loss: 0.0729 - recall_1: 0.1752 - val_loss: 0.2131 - val_recall_1: 0.1768\n",
      "Epoch 59/100\n",
      "926/926 [==============================] - 0s 115us/step - loss: 0.0714 - recall_1: 0.1792 - val_loss: 0.2068 - val_recall_1: 0.1814\n",
      "Epoch 60/100\n",
      "926/926 [==============================] - 0s 116us/step - loss: 0.0726 - recall_1: 0.1825 - val_loss: 0.2231 - val_recall_1: 0.1842\n",
      "Epoch 61/100\n",
      "926/926 [==============================] - 0s 112us/step - loss: 0.0795 - recall_1: 0.1857 - val_loss: 0.2294 - val_recall_1: 0.1879\n",
      "Epoch 62/100\n",
      "926/926 [==============================] - 0s 102us/step - loss: 0.0688 - recall_1: 0.1903 - val_loss: 0.2555 - val_recall_1: 0.1928\n",
      "Epoch 63/100\n",
      "926/926 [==============================] - 0s 103us/step - loss: 0.0754 - recall_1: 0.1948 - val_loss: 0.2597 - val_recall_1: 0.1966\n",
      "Epoch 64/100\n",
      "926/926 [==============================] - 0s 101us/step - loss: 0.0703 - recall_1: 0.1990 - val_loss: 0.3004 - val_recall_1: 0.2010\n",
      "Epoch 65/100\n",
      "926/926 [==============================] - 0s 97us/step - loss: 0.0676 - recall_1: 0.2028 - val_loss: 0.2482 - val_recall_1: 0.2049\n",
      "Epoch 66/100\n",
      "926/926 [==============================] - 0s 98us/step - loss: 0.0709 - recall_1: 0.2070 - val_loss: 0.2485 - val_recall_1: 0.2089\n",
      "Epoch 67/100\n",
      "926/926 [==============================] - 0s 110us/step - loss: 0.0640 - recall_1: 0.2114 - val_loss: 0.2361 - val_recall_1: 0.2138\n",
      "Epoch 68/100\n",
      "926/926 [==============================] - 0s 108us/step - loss: 0.0702 - recall_1: 0.2153 - val_loss: 0.2299 - val_recall_1: 0.2171\n",
      "Epoch 69/100\n",
      "926/926 [==============================] - 0s 113us/step - loss: 0.0634 - recall_1: 0.2186 - val_loss: 0.2345 - val_recall_1: 0.2207\n",
      "Epoch 70/100\n",
      "926/926 [==============================] - 0s 106us/step - loss: 0.0670 - recall_1: 0.2226 - val_loss: 0.3214 - val_recall_1: 0.2247\n",
      "Epoch 71/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0711 - recall_1: 0.2265 - val_loss: 0.3220 - val_recall_1: 0.2283\n",
      "Epoch 72/100\n",
      "926/926 [==============================] - 0s 104us/step - loss: 0.0723 - recall_1: 0.2297 - val_loss: 0.2337 - val_recall_1: 0.2313\n",
      "Epoch 73/100\n",
      "926/926 [==============================] - 0s 101us/step - loss: 0.0626 - recall_1: 0.2334 - val_loss: 0.2466 - val_recall_1: 0.2356\n",
      "Epoch 74/100\n",
      "926/926 [==============================] - 0s 117us/step - loss: 0.0615 - recall_1: 0.2375 - val_loss: 0.2209 - val_recall_1: 0.2396\n",
      "Epoch 75/100\n",
      "926/926 [==============================] - 0s 124us/step - loss: 0.0704 - recall_1: 0.2412 - val_loss: 0.2362 - val_recall_1: 0.2430\n",
      "Epoch 76/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.0634 - recall_1: 0.2447 - val_loss: 0.3073 - val_recall_1: 0.2469\n",
      "Epoch 77/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.0653 - recall_1: 0.2485 - val_loss: 0.2837 - val_recall_1: 0.2503\n",
      "Epoch 78/100\n",
      "926/926 [==============================] - 0s 115us/step - loss: 0.0680 - recall_1: 0.2518 - val_loss: 0.2543 - val_recall_1: 0.2536\n",
      "Epoch 79/100\n",
      "926/926 [==============================] - 0s 116us/step - loss: 0.0635 - recall_1: 0.2556 - val_loss: 0.2424 - val_recall_1: 0.2575\n",
      "Epoch 80/100\n",
      "926/926 [==============================] - 0s 112us/step - loss: 0.0642 - recall_1: 0.2591 - val_loss: 0.2593 - val_recall_1: 0.2606\n",
      "Epoch 81/100\n",
      "926/926 [==============================] - 0s 117us/step - loss: 0.0719 - recall_1: 0.2619 - val_loss: 0.2224 - val_recall_1: 0.2633\n",
      "Epoch 82/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.0651 - recall_1: 0.2647 - val_loss: 0.2056 - val_recall_1: 0.2660\n",
      "Epoch 83/100\n",
      "926/926 [==============================] - 0s 104us/step - loss: 0.0621 - recall_1: 0.2671 - val_loss: 0.2423 - val_recall_1: 0.2688\n",
      "Epoch 84/100\n",
      "926/926 [==============================] - 0s 99us/step - loss: 0.0669 - recall_1: 0.2702 - val_loss: 0.2217 - val_recall_1: 0.2716\n",
      "Epoch 85/100\n",
      "926/926 [==============================] - 0s 100us/step - loss: 0.0685 - recall_1: 0.2726 - val_loss: 0.2066 - val_recall_1: 0.2737\n",
      "Epoch 86/100\n",
      "926/926 [==============================] - 0s 95us/step - loss: 0.0644 - recall_1: 0.2750 - val_loss: 0.2269 - val_recall_1: 0.2763\n",
      "Epoch 87/100\n",
      "926/926 [==============================] - 0s 98us/step - loss: 0.0735 - recall_1: 0.2771 - val_loss: 0.2530 - val_recall_1: 0.2777\n",
      "Epoch 88/100\n",
      "926/926 [==============================] - 0s 111us/step - loss: 0.0684 - recall_1: 0.2788 - val_loss: 0.2583 - val_recall_1: 0.2800\n",
      "Epoch 89/100\n",
      "926/926 [==============================] - 0s 110us/step - loss: 0.0586 - recall_1: 0.2815 - val_loss: 0.2700 - val_recall_1: 0.2828\n",
      "Epoch 90/100\n",
      "926/926 [==============================] - 0s 109us/step - loss: 0.0618 - recall_1: 0.2839 - val_loss: 0.2779 - val_recall_1: 0.2854\n",
      "Epoch 91/100\n",
      "926/926 [==============================] - 0s 112us/step - loss: 0.0651 - recall_1: 0.2867 - val_loss: 0.2516 - val_recall_1: 0.2884\n",
      "Epoch 92/100\n",
      "926/926 [==============================] - 0s 110us/step - loss: 0.0637 - recall_1: 0.2893 - val_loss: 0.2536 - val_recall_1: 0.2908\n",
      "Epoch 93/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.0629 - recall_1: 0.2919 - val_loss: 0.2420 - val_recall_1: 0.2930\n",
      "Epoch 94/100\n",
      "926/926 [==============================] - 0s 101us/step - loss: 0.0635 - recall_1: 0.2942 - val_loss: 0.2333 - val_recall_1: 0.2955\n",
      "Epoch 95/100\n",
      "926/926 [==============================] - 0s 107us/step - loss: 0.0651 - recall_1: 0.2966 - val_loss: 0.2487 - val_recall_1: 0.2978\n",
      "Epoch 96/100\n",
      "926/926 [==============================] - 0s 110us/step - loss: 0.0627 - recall_1: 0.2990 - val_loss: 0.2660 - val_recall_1: 0.3000\n",
      "Epoch 97/100\n",
      "926/926 [==============================] - 0s 105us/step - loss: 0.0546 - recall_1: 0.3014 - val_loss: 0.2638 - val_recall_1: 0.3029\n",
      "Epoch 98/100\n",
      "926/926 [==============================] - 0s 109us/step - loss: 0.0637 - recall_1: 0.3040 - val_loss: 0.2733 - val_recall_1: 0.3055\n",
      "Epoch 99/100\n",
      "926/926 [==============================] - 0s 109us/step - loss: 0.0556 - recall_1: 0.3067 - val_loss: 0.2577 - val_recall_1: 0.3082\n",
      "Epoch 100/100\n",
      "926/926 [==============================] - 0s 110us/step - loss: 0.0610 - recall_1: 0.3097 - val_loss: 0.2890 - val_recall_1: 0.3109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f91b9a373a0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 0.8,\n",
    "                1: 0.2}\n",
    "dropout = 0.5\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'truncated_normal',input_shape = (X.shape[1],)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'truncated_normal'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'truncated_normal'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid' , kernel_initializer= 'truncated_normal'))\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall()])\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 20, validation_split=0.1, class_weight= class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       863\n",
      "           1       1.00      0.31      0.47       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.94      0.65      0.70      1029\n",
      "weighted avg       0.90      0.89      0.86      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       370\n",
      "           1       0.91      0.14      0.24        71\n",
      "\n",
      "    accuracy                           0.86       441\n",
      "   macro avg       0.88      0.57      0.58       441\n",
      "weighted avg       0.87      0.86      0.81       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.9 for x in model.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.9 for x in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_63_input to have shape (55,) but got array with shape (20,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-369-514d1528afe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         x, y, sample_weights = self._standardize_user_data(\n\u001b[0m\u001b[1;32m   1151\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;31m# Standardize the inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         x = training_utils.standardize_input_data(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mfeed_input_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mref_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    142\u001b[0m                             \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_63_input to have shape (55,) but got array with shape (20,)"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 0.75,\n",
    "                1: 0.25}\n",
    "dropout = 0.2\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(200, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 30, validation_split=0.1, class_weight= class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       863\n",
      "           1       1.00      0.67      0.81       166\n",
      "\n",
      "    accuracy                           0.95      1029\n",
      "   macro avg       0.97      0.84      0.89      1029\n",
      "weighted avg       0.95      0.95      0.94      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93       370\n",
      "           1       0.93      0.18      0.31        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.90      0.59      0.62       441\n",
      "weighted avg       0.87      0.87      0.83       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.9 for x in model.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.9 for x in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/200\n",
      "926/926 [==============================] - 0s 361us/step - loss: 0.3671 - precision_4: 0.1980 - val_loss: 0.3483 - val_precision_4: 0.1989\n",
      "Epoch 2/200\n",
      "926/926 [==============================] - 0s 53us/step - loss: 0.1774 - precision_4: 0.1989 - val_loss: 0.2790 - val_precision_4: 0.1989\n",
      "Epoch 3/200\n",
      "926/926 [==============================] - 0s 53us/step - loss: 0.1326 - precision_4: 0.1989 - val_loss: 0.2445 - val_precision_4: 0.1989\n",
      "Epoch 4/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.1173 - precision_4: 0.1989 - val_loss: 0.2265 - val_precision_4: 0.1989\n",
      "Epoch 5/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1116 - precision_4: 0.1989 - val_loss: 0.2070 - val_precision_4: 0.1989\n",
      "Epoch 6/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1091 - precision_4: 0.1989 - val_loss: 0.1890 - val_precision_4: 0.1989\n",
      "Epoch 7/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.1043 - precision_4: 0.1989 - val_loss: 0.1658 - val_precision_4: 0.1989\n",
      "Epoch 8/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1035 - precision_4: 0.1989 - val_loss: 0.1414 - val_precision_4: 0.1989\n",
      "Epoch 9/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.1004 - precision_4: 0.1989 - val_loss: 0.1336 - val_precision_4: 0.1989\n",
      "Epoch 10/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.1002 - precision_4: 0.1989 - val_loss: 0.1269 - val_precision_4: 0.1989\n",
      "Epoch 11/200\n",
      "926/926 [==============================] - 0s 46us/step - loss: 0.0979 - precision_4: 0.2030 - val_loss: 0.1213 - val_precision_4: 0.2034\n",
      "Epoch 12/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0941 - precision_4: 0.2034 - val_loss: 0.1224 - val_precision_4: 0.2034\n",
      "Epoch 13/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0930 - precision_4: 0.2034 - val_loss: 0.1192 - val_precision_4: 0.2034\n",
      "Epoch 14/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0901 - precision_4: 0.2155 - val_loss: 0.1172 - val_precision_4: 0.2242\n",
      "Epoch 15/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0895 - precision_4: 0.2321 - val_loss: 0.1138 - val_precision_4: 0.2419\n",
      "Epoch 16/200\n",
      "926/926 [==============================] - 0s 47us/step - loss: 0.0899 - precision_4: 0.2435 - val_loss: 0.1182 - val_precision_4: 0.2549\n",
      "Epoch 17/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0846 - precision_4: 0.2929 - val_loss: 0.1149 - val_precision_4: 0.3294\n",
      "Epoch 18/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0843 - precision_4: 0.3538 - val_loss: 0.1155 - val_precision_4: 0.3795\n",
      "Epoch 19/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0835 - precision_4: 0.3937 - val_loss: 0.1148 - val_precision_4: 0.4179\n",
      "Epoch 20/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0829 - precision_4: 0.4445 - val_loss: 0.1179 - val_precision_4: 0.4797\n",
      "Epoch 21/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0839 - precision_4: 0.5033 - val_loss: 0.1184 - val_precision_4: 0.5300\n",
      "Epoch 22/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0819 - precision_4: 0.5471 - val_loss: 0.1194 - val_precision_4: 0.5740\n",
      "Epoch 23/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0825 - precision_4: 0.5928 - val_loss: 0.1223 - val_precision_4: 0.6121\n",
      "Epoch 24/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0797 - precision_4: 0.6276 - val_loss: 0.1227 - val_precision_4: 0.6435\n",
      "Epoch 25/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0782 - precision_4: 0.6601 - val_loss: 0.1177 - val_precision_4: 0.6736\n",
      "Epoch 26/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0770 - precision_4: 0.6893 - val_loss: 0.1221 - val_precision_4: 0.7047\n",
      "Epoch 27/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0766 - precision_4: 0.7161 - val_loss: 0.1156 - val_precision_4: 0.7281\n",
      "Epoch 28/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0734 - precision_4: 0.7395 - val_loss: 0.1129 - val_precision_4: 0.7489\n",
      "Epoch 29/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0757 - precision_4: 0.7561 - val_loss: 0.1221 - val_precision_4: 0.7663\n",
      "Epoch 30/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0709 - precision_4: 0.7744 - val_loss: 0.1239 - val_precision_4: 0.7813\n",
      "Epoch 31/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0727 - precision_4: 0.7885 - val_loss: 0.1214 - val_precision_4: 0.7939\n",
      "Epoch 32/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0717 - precision_4: 0.8023 - val_loss: 0.1131 - val_precision_4: 0.8071\n",
      "Epoch 33/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0697 - precision_4: 0.8131 - val_loss: 0.1188 - val_precision_4: 0.8179\n",
      "Epoch 34/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0715 - precision_4: 0.8231 - val_loss: 0.1197 - val_precision_4: 0.8279\n",
      "Epoch 35/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0710 - precision_4: 0.8327 - val_loss: 0.1173 - val_precision_4: 0.8366\n",
      "Epoch 36/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0684 - precision_4: 0.8405 - val_loss: 0.1155 - val_precision_4: 0.8439\n",
      "Epoch 37/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0710 - precision_4: 0.8475 - val_loss: 0.1156 - val_precision_4: 0.8506\n",
      "Epoch 38/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0693 - precision_4: 0.8533 - val_loss: 0.1137 - val_precision_4: 0.8551\n",
      "Epoch 39/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0671 - precision_4: 0.8592 - val_loss: 0.1148 - val_precision_4: 0.8616\n",
      "Epoch 40/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0675 - precision_4: 0.8648 - val_loss: 0.1173 - val_precision_4: 0.8666\n",
      "Epoch 41/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0677 - precision_4: 0.8698 - val_loss: 0.1158 - val_precision_4: 0.8710\n",
      "Epoch 42/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0678 - precision_4: 0.8737 - val_loss: 0.1171 - val_precision_4: 0.8748\n",
      "Epoch 43/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0653 - precision_4: 0.8775 - val_loss: 0.1180 - val_precision_4: 0.8791\n",
      "Epoch 44/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0611 - precision_4: 0.8821 - val_loss: 0.1141 - val_precision_4: 0.8839\n",
      "Epoch 45/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0620 - precision_4: 0.8864 - val_loss: 0.1148 - val_precision_4: 0.8883\n",
      "Epoch 46/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0655 - precision_4: 0.8905 - val_loss: 0.1171 - val_precision_4: 0.8918\n",
      "Epoch 47/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0589 - precision_4: 0.8942 - val_loss: 0.1199 - val_precision_4: 0.8953\n",
      "Epoch 48/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0627 - precision_4: 0.8970 - val_loss: 0.1219 - val_precision_4: 0.8977\n",
      "Epoch 49/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0592 - precision_4: 0.8992 - val_loss: 0.1238 - val_precision_4: 0.9004\n",
      "Epoch 50/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0603 - precision_4: 0.9018 - val_loss: 0.1221 - val_precision_4: 0.9029\n",
      "Epoch 51/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0615 - precision_4: 0.9037 - val_loss: 0.1270 - val_precision_4: 0.9045\n",
      "Epoch 52/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0583 - precision_4: 0.9059 - val_loss: 0.1273 - val_precision_4: 0.9066\n",
      "Epoch 53/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0585 - precision_4: 0.9081 - val_loss: 0.1262 - val_precision_4: 0.9090\n",
      "Epoch 54/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0557 - precision_4: 0.9103 - val_loss: 0.1290 - val_precision_4: 0.9105\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926/926 [==============================] - 0s 39us/step - loss: 0.0554 - precision_4: 0.9116 - val_loss: 0.1330 - val_precision_4: 0.9120\n",
      "Epoch 56/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0572 - precision_4: 0.9128 - val_loss: 0.1270 - val_precision_4: 0.9134\n",
      "Epoch 57/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0605 - precision_4: 0.9142 - val_loss: 0.1312 - val_precision_4: 0.9143\n",
      "Epoch 58/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0574 - precision_4: 0.9153 - val_loss: 0.1385 - val_precision_4: 0.9153\n",
      "Epoch 59/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0571 - precision_4: 0.9160 - val_loss: 0.1288 - val_precision_4: 0.9161\n",
      "Epoch 60/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0543 - precision_4: 0.9170 - val_loss: 0.1315 - val_precision_4: 0.9176\n",
      "Epoch 61/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0614 - precision_4: 0.9184 - val_loss: 0.1357 - val_precision_4: 0.9184\n",
      "Epoch 62/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0540 - precision_4: 0.9193 - val_loss: 0.1323 - val_precision_4: 0.9198\n",
      "Epoch 63/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0519 - precision_4: 0.9207 - val_loss: 0.1329 - val_precision_4: 0.9211\n",
      "Epoch 64/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0503 - precision_4: 0.9220 - val_loss: 0.1387 - val_precision_4: 0.9224\n",
      "Epoch 65/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0503 - precision_4: 0.9232 - val_loss: 0.1352 - val_precision_4: 0.9231\n",
      "Epoch 66/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0517 - precision_4: 0.9237 - val_loss: 0.1374 - val_precision_4: 0.9236\n",
      "Epoch 67/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0560 - precision_4: 0.9240 - val_loss: 0.1404 - val_precision_4: 0.9237\n",
      "Epoch 68/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0520 - precision_4: 0.9241 - val_loss: 0.1391 - val_precision_4: 0.9244\n",
      "Epoch 69/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0486 - precision_4: 0.9250 - val_loss: 0.1304 - val_precision_4: 0.9254\n",
      "Epoch 70/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0495 - precision_4: 0.9260 - val_loss: 0.1432 - val_precision_4: 0.9255\n",
      "Epoch 71/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0490 - precision_4: 0.9258 - val_loss: 0.1371 - val_precision_4: 0.9259\n",
      "Epoch 72/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0468 - precision_4: 0.9268 - val_loss: 0.1379 - val_precision_4: 0.9270\n",
      "Epoch 73/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0521 - precision_4: 0.9270 - val_loss: 0.1417 - val_precision_4: 0.9266\n",
      "Epoch 74/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0486 - precision_4: 0.9273 - val_loss: 0.1441 - val_precision_4: 0.9269\n",
      "Epoch 75/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0527 - precision_4: 0.9274 - val_loss: 0.1439 - val_precision_4: 0.9272\n",
      "Epoch 76/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0483 - precision_4: 0.9275 - val_loss: 0.1364 - val_precision_4: 0.9275\n",
      "Epoch 77/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0471 - precision_4: 0.9283 - val_loss: 0.1391 - val_precision_4: 0.9282\n",
      "Epoch 78/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0483 - precision_4: 0.9286 - val_loss: 0.1498 - val_precision_4: 0.9286\n",
      "Epoch 79/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0476 - precision_4: 0.9291 - val_loss: 0.1511 - val_precision_4: 0.9290\n",
      "Epoch 80/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0480 - precision_4: 0.9293 - val_loss: 0.1503 - val_precision_4: 0.9294\n",
      "Epoch 81/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0459 - precision_4: 0.9300 - val_loss: 0.1532 - val_precision_4: 0.9299\n",
      "Epoch 82/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0456 - precision_4: 0.9304 - val_loss: 0.1546 - val_precision_4: 0.9303\n",
      "Epoch 83/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0470 - precision_4: 0.9307 - val_loss: 0.1450 - val_precision_4: 0.9305\n",
      "Epoch 84/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0468 - precision_4: 0.9311 - val_loss: 0.1483 - val_precision_4: 0.9310\n",
      "Epoch 85/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0429 - precision_4: 0.9314 - val_loss: 0.1450 - val_precision_4: 0.9316\n",
      "Epoch 86/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0445 - precision_4: 0.9323 - val_loss: 0.1499 - val_precision_4: 0.9323\n",
      "Epoch 87/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0422 - precision_4: 0.9327 - val_loss: 0.1544 - val_precision_4: 0.9325\n",
      "Epoch 88/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0437 - precision_4: 0.9328 - val_loss: 0.1544 - val_precision_4: 0.9327\n",
      "Epoch 89/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0458 - precision_4: 0.9328 - val_loss: 0.1553 - val_precision_4: 0.9325\n",
      "Epoch 90/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0443 - precision_4: 0.9329 - val_loss: 0.1658 - val_precision_4: 0.9326\n",
      "Epoch 91/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0440 - precision_4: 0.9331 - val_loss: 0.1485 - val_precision_4: 0.9331\n",
      "Epoch 92/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0376 - precision_4: 0.9337 - val_loss: 0.1470 - val_precision_4: 0.9339\n",
      "Epoch 93/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0415 - precision_4: 0.9343 - val_loss: 0.1526 - val_precision_4: 0.9345\n",
      "Epoch 94/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0422 - precision_4: 0.9348 - val_loss: 0.1606 - val_precision_4: 0.9348\n",
      "Epoch 95/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0410 - precision_4: 0.9350 - val_loss: 0.1518 - val_precision_4: 0.9349\n",
      "Epoch 96/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0446 - precision_4: 0.9350 - val_loss: 0.1493 - val_precision_4: 0.9350\n",
      "Epoch 97/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0403 - precision_4: 0.9351 - val_loss: 0.1578 - val_precision_4: 0.9351\n",
      "Epoch 98/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0418 - precision_4: 0.9354 - val_loss: 0.1507 - val_precision_4: 0.9351\n",
      "Epoch 99/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0448 - precision_4: 0.9353 - val_loss: 0.1545 - val_precision_4: 0.9352\n",
      "Epoch 100/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0382 - precision_4: 0.9357 - val_loss: 0.1556 - val_precision_4: 0.9359\n",
      "Epoch 101/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0416 - precision_4: 0.9364 - val_loss: 0.1569 - val_precision_4: 0.9364\n",
      "Epoch 102/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0397 - precision_4: 0.9366 - val_loss: 0.1658 - val_precision_4: 0.9361\n",
      "Epoch 103/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0406 - precision_4: 0.9364 - val_loss: 0.1586 - val_precision_4: 0.9363\n",
      "Epoch 104/200\n",
      "926/926 [==============================] - 0s 51us/step - loss: 0.0407 - precision_4: 0.9365 - val_loss: 0.1646 - val_precision_4: 0.9364\n",
      "Epoch 105/200\n",
      "926/926 [==============================] - 0s 49us/step - loss: 0.0358 - precision_4: 0.9366 - val_loss: 0.1505 - val_precision_4: 0.9368\n",
      "Epoch 106/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0381 - precision_4: 0.9372 - val_loss: 0.1628 - val_precision_4: 0.9371\n",
      "Epoch 107/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0382 - precision_4: 0.9375 - val_loss: 0.1688 - val_precision_4: 0.9375\n",
      "Epoch 108/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0392 - precision_4: 0.9378 - val_loss: 0.1577 - val_precision_4: 0.9375\n",
      "Epoch 109/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0386 - precision_4: 0.9379 - val_loss: 0.1576 - val_precision_4: 0.9379\n",
      "Epoch 110/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0341 - precision_4: 0.9383 - val_loss: 0.1594 - val_precision_4: 0.9384\n",
      "Epoch 111/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0352 - precision_4: 0.9387 - val_loss: 0.1709 - val_precision_4: 0.9387\n",
      "Epoch 112/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0333 - precision_4: 0.9391 - val_loss: 0.1650 - val_precision_4: 0.9391\n",
      "Epoch 113/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0377 - precision_4: 0.9394 - val_loss: 0.1686 - val_precision_4: 0.9391\n",
      "Epoch 114/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0419 - precision_4: 0.9395 - val_loss: 0.1620 - val_precision_4: 0.9394\n",
      "Epoch 115/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0380 - precision_4: 0.9396 - val_loss: 0.1642 - val_precision_4: 0.9396\n",
      "Epoch 116/200\n",
      "926/926 [==============================] - 0s 52us/step - loss: 0.0379 - precision_4: 0.9397 - val_loss: 0.1668 - val_precision_4: 0.9397\n",
      "Epoch 117/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0395 - precision_4: 0.9399 - val_loss: 0.1594 - val_precision_4: 0.9398\n",
      "Epoch 118/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0381 - precision_4: 0.9400 - val_loss: 0.1699 - val_precision_4: 0.9400\n",
      "Epoch 119/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0292 - precision_4: 0.9404 - val_loss: 0.1788 - val_precision_4: 0.9404\n",
      "Epoch 120/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0344 - precision_4: 0.9406 - val_loss: 0.1785 - val_precision_4: 0.9406\n",
      "Epoch 121/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0365 - precision_4: 0.9407 - val_loss: 0.1789 - val_precision_4: 0.9407\n",
      "Epoch 122/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0331 - precision_4: 0.9411 - val_loss: 0.1760 - val_precision_4: 0.9412\n",
      "Epoch 123/200\n",
      "926/926 [==============================] - 0s 46us/step - loss: 0.0330 - precision_4: 0.9415 - val_loss: 0.1721 - val_precision_4: 0.9415\n",
      "Epoch 124/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0326 - precision_4: 0.9417 - val_loss: 0.1752 - val_precision_4: 0.9415\n",
      "Epoch 125/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0387 - precision_4: 0.9417 - val_loss: 0.1738 - val_precision_4: 0.9415\n",
      "Epoch 126/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0355 - precision_4: 0.9417 - val_loss: 0.1789 - val_precision_4: 0.9417\n",
      "Epoch 127/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0356 - precision_4: 0.9419 - val_loss: 0.1742 - val_precision_4: 0.9418\n",
      "Epoch 128/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0358 - precision_4: 0.9420 - val_loss: 0.1784 - val_precision_4: 0.9417\n",
      "Epoch 129/200\n",
      "926/926 [==============================] - 0s 47us/step - loss: 0.0363 - precision_4: 0.9420 - val_loss: 0.1752 - val_precision_4: 0.9418\n",
      "Epoch 130/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0328 - precision_4: 0.9421 - val_loss: 0.1848 - val_precision_4: 0.9422\n",
      "Epoch 131/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0277 - precision_4: 0.9424 - val_loss: 0.1734 - val_precision_4: 0.9424\n",
      "Epoch 132/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0324 - precision_4: 0.9426 - val_loss: 0.1674 - val_precision_4: 0.9425\n",
      "Epoch 133/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0327 - precision_4: 0.9427 - val_loss: 0.1657 - val_precision_4: 0.9426\n",
      "Epoch 134/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0388 - precision_4: 0.9427 - val_loss: 0.1622 - val_precision_4: 0.9423\n",
      "Epoch 135/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0349 - precision_4: 0.9425 - val_loss: 0.1634 - val_precision_4: 0.9425\n",
      "Epoch 136/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0299 - precision_4: 0.9427 - val_loss: 0.1869 - val_precision_4: 0.9426\n",
      "Epoch 137/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0331 - precision_4: 0.9428 - val_loss: 0.1776 - val_precision_4: 0.9428\n",
      "Epoch 138/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0337 - precision_4: 0.9429 - val_loss: 0.1620 - val_precision_4: 0.9428\n",
      "Epoch 139/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0356 - precision_4: 0.9431 - val_loss: 0.1741 - val_precision_4: 0.9432\n",
      "Epoch 140/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0336 - precision_4: 0.9434 - val_loss: 0.1781 - val_precision_4: 0.9434\n",
      "Epoch 141/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0275 - precision_4: 0.9436 - val_loss: 0.1814 - val_precision_4: 0.9437\n",
      "Epoch 142/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0247 - precision_4: 0.9440 - val_loss: 0.1758 - val_precision_4: 0.9439\n",
      "Epoch 143/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0293 - precision_4: 0.9442 - val_loss: 0.1686 - val_precision_4: 0.9442\n",
      "Epoch 144/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0391 - precision_4: 0.9444 - val_loss: 0.1938 - val_precision_4: 0.9443\n",
      "Epoch 145/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0327 - precision_4: 0.9445 - val_loss: 0.1782 - val_precision_4: 0.9445\n",
      "Epoch 146/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0315 - precision_4: 0.9447 - val_loss: 0.1746 - val_precision_4: 0.9446\n",
      "Epoch 147/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0311 - precision_4: 0.9449 - val_loss: 0.1711 - val_precision_4: 0.9449\n",
      "Epoch 148/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0295 - precision_4: 0.9450 - val_loss: 0.1840 - val_precision_4: 0.9450\n",
      "Epoch 149/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0321 - precision_4: 0.9451 - val_loss: 0.1930 - val_precision_4: 0.9449\n",
      "Epoch 150/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0279 - precision_4: 0.9450 - val_loss: 0.1848 - val_precision_4: 0.9450\n",
      "Epoch 151/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0255 - precision_4: 0.9453 - val_loss: 0.1716 - val_precision_4: 0.9452\n",
      "Epoch 152/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0334 - precision_4: 0.9453 - val_loss: 0.1802 - val_precision_4: 0.9452\n",
      "Epoch 153/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0277 - precision_4: 0.9454 - val_loss: 0.1874 - val_precision_4: 0.9454\n",
      "Epoch 154/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0361 - precision_4: 0.9454 - val_loss: 0.2053 - val_precision_4: 0.9452\n",
      "Epoch 155/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0290 - precision_4: 0.9453 - val_loss: 0.2066 - val_precision_4: 0.9452\n",
      "Epoch 156/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0270 - precision_4: 0.9454 - val_loss: 0.2031 - val_precision_4: 0.9453\n",
      "Epoch 157/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0286 - precision_4: 0.9454 - val_loss: 0.1855 - val_precision_4: 0.9454\n",
      "Epoch 158/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0325 - precision_4: 0.9455 - val_loss: 0.1816 - val_precision_4: 0.9455\n",
      "Epoch 159/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0287 - precision_4: 0.9456 - val_loss: 0.1773 - val_precision_4: 0.9456\n",
      "Epoch 160/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0334 - precision_4: 0.9456 - val_loss: 0.1684 - val_precision_4: 0.9457\n",
      "Epoch 161/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0283 - precision_4: 0.9459 - val_loss: 0.1680 - val_precision_4: 0.9461\n",
      "Epoch 162/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0353 - precision_4: 0.9462 - val_loss: 0.1692 - val_precision_4: 0.9461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0268 - precision_4: 0.9462 - val_loss: 0.1968 - val_precision_4: 0.9462\n",
      "Epoch 164/200\n",
      "926/926 [==============================] - 0s 46us/step - loss: 0.0268 - precision_4: 0.9463 - val_loss: 0.1772 - val_precision_4: 0.9463\n",
      "Epoch 165/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0283 - precision_4: 0.9465 - val_loss: 0.1806 - val_precision_4: 0.9464\n",
      "Epoch 166/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0298 - precision_4: 0.9463 - val_loss: 0.2021 - val_precision_4: 0.9464\n",
      "Epoch 167/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0329 - precision_4: 0.9466 - val_loss: 0.2051 - val_precision_4: 0.9466\n",
      "Epoch 168/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0276 - precision_4: 0.9467 - val_loss: 0.1848 - val_precision_4: 0.9466\n",
      "Epoch 169/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0287 - precision_4: 0.9468 - val_loss: 0.1951 - val_precision_4: 0.9467\n",
      "Epoch 170/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0270 - precision_4: 0.9469 - val_loss: 0.2001 - val_precision_4: 0.9469\n",
      "Epoch 171/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0300 - precision_4: 0.9470 - val_loss: 0.1925 - val_precision_4: 0.9469\n",
      "Epoch 172/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0279 - precision_4: 0.9470 - val_loss: 0.1915 - val_precision_4: 0.9470\n",
      "Epoch 173/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0256 - precision_4: 0.9471 - val_loss: 0.1966 - val_precision_4: 0.9469\n",
      "Epoch 174/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0261 - precision_4: 0.9470 - val_loss: 0.1784 - val_precision_4: 0.9468\n",
      "Epoch 175/200\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0363 - precision_4: 0.9468 - val_loss: 0.1898 - val_precision_4: 0.9466\n",
      "Epoch 176/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0281 - precision_4: 0.9467 - val_loss: 0.1819 - val_precision_4: 0.9466\n",
      "Epoch 177/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0248 - precision_4: 0.9468 - val_loss: 0.1841 - val_precision_4: 0.9467\n",
      "Epoch 178/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0243 - precision_4: 0.9468 - val_loss: 0.1936 - val_precision_4: 0.9468\n",
      "Epoch 179/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0296 - precision_4: 0.9468 - val_loss: 0.2025 - val_precision_4: 0.9467\n",
      "Epoch 180/200\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0304 - precision_4: 0.9467 - val_loss: 0.1872 - val_precision_4: 0.9467\n",
      "Epoch 181/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0277 - precision_4: 0.9467 - val_loss: 0.1742 - val_precision_4: 0.9467\n",
      "Epoch 182/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0287 - precision_4: 0.9467 - val_loss: 0.1839 - val_precision_4: 0.9467\n",
      "Epoch 183/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0257 - precision_4: 0.9469 - val_loss: 0.1904 - val_precision_4: 0.9468\n",
      "Epoch 184/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0241 - precision_4: 0.9469 - val_loss: 0.1852 - val_precision_4: 0.9469\n",
      "Epoch 185/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0264 - precision_4: 0.9470 - val_loss: 0.1890 - val_precision_4: 0.9470\n",
      "Epoch 186/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0296 - precision_4: 0.9470 - val_loss: 0.1862 - val_precision_4: 0.9470\n",
      "Epoch 187/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0277 - precision_4: 0.9471 - val_loss: 0.2123 - val_precision_4: 0.9470\n",
      "Epoch 188/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0235 - precision_4: 0.9472 - val_loss: 0.2124 - val_precision_4: 0.9472\n",
      "Epoch 189/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0239 - precision_4: 0.9472 - val_loss: 0.2148 - val_precision_4: 0.9472\n",
      "Epoch 190/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0296 - precision_4: 0.9471 - val_loss: 0.2166 - val_precision_4: 0.9471\n",
      "Epoch 191/200\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0303 - precision_4: 0.9471 - val_loss: 0.1978 - val_precision_4: 0.9471\n",
      "Epoch 192/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0259 - precision_4: 0.9472 - val_loss: 0.1952 - val_precision_4: 0.9472\n",
      "Epoch 193/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0290 - precision_4: 0.9473 - val_loss: 0.1787 - val_precision_4: 0.9472\n",
      "Epoch 194/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0260 - precision_4: 0.9473 - val_loss: 0.1909 - val_precision_4: 0.9473\n",
      "Epoch 195/200\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0250 - precision_4: 0.9474 - val_loss: 0.1977 - val_precision_4: 0.9473\n",
      "Epoch 196/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0259 - precision_4: 0.9473 - val_loss: 0.2114 - val_precision_4: 0.9472\n",
      "Epoch 197/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0243 - precision_4: 0.9473 - val_loss: 0.1913 - val_precision_4: 0.9473\n",
      "Epoch 198/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0221 - precision_4: 0.9473 - val_loss: 0.1981 - val_precision_4: 0.9472\n",
      "Epoch 199/200\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0231 - precision_4: 0.9472 - val_loss: 0.2026 - val_precision_4: 0.9472\n",
      "Epoch 200/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0240 - precision_4: 0.9473 - val_loss: 0.1967 - val_precision_4: 0.9474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f91d9a1f0a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 0.8,\n",
    "                1: 0.2}\n",
    "dropout = 0.4\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'truncated_normal',input_shape = (X.shape[1],)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid', kernel_initializer= 'truncated_normal'))\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "model.fit(X_train, y_train, epochs = 200, batch_size = 30, validation_split=0.1, class_weight= class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       863\n",
      "           1       1.00      0.53      0.69       166\n",
      "\n",
      "    accuracy                           0.92      1029\n",
      "   macro avg       0.96      0.77      0.82      1029\n",
      "weighted avg       0.93      0.92      0.91      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       370\n",
      "           1       0.76      0.18      0.30        71\n",
      "\n",
      "    accuracy                           0.86       441\n",
      "   macro avg       0.81      0.59      0.61       441\n",
      "weighted avg       0.85      0.86      0.82       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.91 for x in model.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.91 for x in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/200\n",
      "926/926 [==============================] - 0s 297us/step - loss: 0.1942 - accuracy: 0.8164 - val_loss: 0.1570 - val_accuracy: 0.7961\n",
      "Epoch 2/200\n",
      "926/926 [==============================] - 0s 51us/step - loss: 0.1334 - accuracy: 0.8434 - val_loss: 0.1582 - val_accuracy: 0.7961\n",
      "Epoch 3/200\n",
      "926/926 [==============================] - 0s 48us/step - loss: 0.1269 - accuracy: 0.8434 - val_loss: 0.1686 - val_accuracy: 0.7961\n",
      "Epoch 4/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.1246 - accuracy: 0.8434 - val_loss: 0.1732 - val_accuracy: 0.7961\n",
      "Epoch 5/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1226 - accuracy: 0.8434 - val_loss: 0.1716 - val_accuracy: 0.7961\n",
      "Epoch 6/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1168 - accuracy: 0.8445 - val_loss: 0.1652 - val_accuracy: 0.7961\n",
      "Epoch 7/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.1158 - accuracy: 0.8456 - val_loss: 0.1716 - val_accuracy: 0.7961\n",
      "Epoch 8/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.1166 - accuracy: 0.8434 - val_loss: 0.1730 - val_accuracy: 0.8155\n",
      "Epoch 9/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.1122 - accuracy: 0.8434 - val_loss: 0.1703 - val_accuracy: 0.8252\n",
      "Epoch 10/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.1112 - accuracy: 0.8434 - val_loss: 0.1739 - val_accuracy: 0.8252\n",
      "Epoch 11/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.1139 - accuracy: 0.8499 - val_loss: 0.1679 - val_accuracy: 0.8252\n",
      "Epoch 12/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1085 - accuracy: 0.8456 - val_loss: 0.1746 - val_accuracy: 0.8252\n",
      "Epoch 13/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.1090 - accuracy: 0.8434 - val_loss: 0.1790 - val_accuracy: 0.8350\n",
      "Epoch 14/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.1042 - accuracy: 0.8456 - val_loss: 0.1846 - val_accuracy: 0.8932\n",
      "Epoch 15/200\n",
      "926/926 [==============================] - 0s 47us/step - loss: 0.1078 - accuracy: 0.8445 - val_loss: 0.1823 - val_accuracy: 0.9126\n",
      "Epoch 16/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1088 - accuracy: 0.8467 - val_loss: 0.1708 - val_accuracy: 0.9029\n",
      "Epoch 17/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1015 - accuracy: 0.8499 - val_loss: 0.1748 - val_accuracy: 0.9029\n",
      "Epoch 18/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.1033 - accuracy: 0.8499 - val_loss: 0.1716 - val_accuracy: 0.9029\n",
      "Epoch 19/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.1027 - accuracy: 0.8531 - val_loss: 0.1727 - val_accuracy: 0.9029\n",
      "Epoch 20/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1038 - accuracy: 0.8553 - val_loss: 0.1738 - val_accuracy: 0.9029\n",
      "Epoch 21/200\n",
      "926/926 [==============================] - 0s 46us/step - loss: 0.1030 - accuracy: 0.8510 - val_loss: 0.1705 - val_accuracy: 0.9029\n",
      "Epoch 22/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0972 - accuracy: 0.8521 - val_loss: 0.1659 - val_accuracy: 0.9029\n",
      "Epoch 23/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0981 - accuracy: 0.8585 - val_loss: 0.1658 - val_accuracy: 0.9029\n",
      "Epoch 24/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0963 - accuracy: 0.8553 - val_loss: 0.1636 - val_accuracy: 0.9029\n",
      "Epoch 25/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.1002 - accuracy: 0.8542 - val_loss: 0.1677 - val_accuracy: 0.9029\n",
      "Epoch 26/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0971 - accuracy: 0.8553 - val_loss: 0.1653 - val_accuracy: 0.9126\n",
      "Epoch 27/200\n",
      "926/926 [==============================] - 0s 47us/step - loss: 0.0964 - accuracy: 0.8575 - val_loss: 0.1575 - val_accuracy: 0.9029\n",
      "Epoch 28/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0963 - accuracy: 0.8564 - val_loss: 0.1596 - val_accuracy: 0.9126\n",
      "Epoch 29/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0975 - accuracy: 0.8564 - val_loss: 0.1580 - val_accuracy: 0.9126\n",
      "Epoch 30/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0967 - accuracy: 0.8618 - val_loss: 0.1513 - val_accuracy: 0.9029\n",
      "Epoch 31/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0963 - accuracy: 0.8596 - val_loss: 0.1550 - val_accuracy: 0.9029\n",
      "Epoch 32/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0934 - accuracy: 0.8618 - val_loss: 0.1619 - val_accuracy: 0.8932\n",
      "Epoch 33/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0929 - accuracy: 0.8661 - val_loss: 0.1697 - val_accuracy: 0.9029\n",
      "Epoch 34/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0930 - accuracy: 0.8639 - val_loss: 0.1716 - val_accuracy: 0.8932\n",
      "Epoch 35/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0905 - accuracy: 0.8715 - val_loss: 0.1741 - val_accuracy: 0.8835\n",
      "Epoch 36/200\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0931 - accuracy: 0.8661 - val_loss: 0.1620 - val_accuracy: 0.8932\n",
      "Epoch 37/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0907 - accuracy: 0.8661 - val_loss: 0.1628 - val_accuracy: 0.9029\n",
      "Epoch 38/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0895 - accuracy: 0.8661 - val_loss: 0.1576 - val_accuracy: 0.9029\n",
      "Epoch 39/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0902 - accuracy: 0.8661 - val_loss: 0.1585 - val_accuracy: 0.8932\n",
      "Epoch 40/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0894 - accuracy: 0.8715 - val_loss: 0.1548 - val_accuracy: 0.8932\n",
      "Epoch 41/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0882 - accuracy: 0.8639 - val_loss: 0.1529 - val_accuracy: 0.8932\n",
      "Epoch 42/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0874 - accuracy: 0.8737 - val_loss: 0.1553 - val_accuracy: 0.9029\n",
      "Epoch 43/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0915 - accuracy: 0.8596 - val_loss: 0.1481 - val_accuracy: 0.9029\n",
      "Epoch 44/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0875 - accuracy: 0.8693 - val_loss: 0.1468 - val_accuracy: 0.9126\n",
      "Epoch 45/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0851 - accuracy: 0.8769 - val_loss: 0.1414 - val_accuracy: 0.9029\n",
      "Epoch 46/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0888 - accuracy: 0.8661 - val_loss: 0.1389 - val_accuracy: 0.9029\n",
      "Epoch 47/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0854 - accuracy: 0.8726 - val_loss: 0.1392 - val_accuracy: 0.9029\n",
      "Epoch 48/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0829 - accuracy: 0.8737 - val_loss: 0.1397 - val_accuracy: 0.9320\n",
      "Epoch 49/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0845 - accuracy: 0.8769 - val_loss: 0.1403 - val_accuracy: 0.9029\n",
      "Epoch 50/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0858 - accuracy: 0.8704 - val_loss: 0.1368 - val_accuracy: 0.9126\n",
      "Epoch 51/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0828 - accuracy: 0.8758 - val_loss: 0.1383 - val_accuracy: 0.9126\n",
      "Epoch 52/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0843 - accuracy: 0.8812 - val_loss: 0.1410 - val_accuracy: 0.9223\n",
      "Epoch 53/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0881 - accuracy: 0.8639 - val_loss: 0.1452 - val_accuracy: 0.9126\n",
      "Epoch 54/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0835 - accuracy: 0.8715 - val_loss: 0.1468 - val_accuracy: 0.9126\n",
      "Epoch 55/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0803 - accuracy: 0.8823 - val_loss: 0.1515 - val_accuracy: 0.9029\n",
      "Epoch 56/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0819 - accuracy: 0.8812 - val_loss: 0.1408 - val_accuracy: 0.9029\n",
      "Epoch 57/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0843 - accuracy: 0.8769 - val_loss: 0.1383 - val_accuracy: 0.9029\n",
      "Epoch 58/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0794 - accuracy: 0.8823 - val_loss: 0.1394 - val_accuracy: 0.9029\n",
      "Epoch 59/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0809 - accuracy: 0.8726 - val_loss: 0.1394 - val_accuracy: 0.9029\n",
      "Epoch 60/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0827 - accuracy: 0.8737 - val_loss: 0.1403 - val_accuracy: 0.9029\n",
      "Epoch 61/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0806 - accuracy: 0.8909 - val_loss: 0.1520 - val_accuracy: 0.9029\n",
      "Epoch 62/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0782 - accuracy: 0.8855 - val_loss: 0.1361 - val_accuracy: 0.9126\n",
      "Epoch 63/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0756 - accuracy: 0.8790 - val_loss: 0.1403 - val_accuracy: 0.9029\n",
      "Epoch 64/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0789 - accuracy: 0.8942 - val_loss: 0.1381 - val_accuracy: 0.9126\n",
      "Epoch 65/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0834 - accuracy: 0.8823 - val_loss: 0.1373 - val_accuracy: 0.9126\n",
      "Epoch 66/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0757 - accuracy: 0.8898 - val_loss: 0.1342 - val_accuracy: 0.9029\n",
      "Epoch 67/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0799 - accuracy: 0.8844 - val_loss: 0.1348 - val_accuracy: 0.9126\n",
      "Epoch 68/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0763 - accuracy: 0.8920 - val_loss: 0.1347 - val_accuracy: 0.9126\n",
      "Epoch 69/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0726 - accuracy: 0.8877 - val_loss: 0.1376 - val_accuracy: 0.9029\n",
      "Epoch 70/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0758 - accuracy: 0.8877 - val_loss: 0.1412 - val_accuracy: 0.8835\n",
      "Epoch 71/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0747 - accuracy: 0.8888 - val_loss: 0.1354 - val_accuracy: 0.9029\n",
      "Epoch 72/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0750 - accuracy: 0.8920 - val_loss: 0.1305 - val_accuracy: 0.9126\n",
      "Epoch 73/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0806 - accuracy: 0.8931 - val_loss: 0.1327 - val_accuracy: 0.9029\n",
      "Epoch 74/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0760 - accuracy: 0.8974 - val_loss: 0.1320 - val_accuracy: 0.9029\n",
      "Epoch 75/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0742 - accuracy: 0.8888 - val_loss: 0.1319 - val_accuracy: 0.9029\n",
      "Epoch 76/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0758 - accuracy: 0.8920 - val_loss: 0.1292 - val_accuracy: 0.9029\n",
      "Epoch 77/200\n",
      "926/926 [==============================] - 0s 46us/step - loss: 0.0727 - accuracy: 0.8931 - val_loss: 0.1292 - val_accuracy: 0.9029\n",
      "Epoch 78/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0774 - accuracy: 0.8942 - val_loss: 0.1310 - val_accuracy: 0.9126\n",
      "Epoch 79/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0743 - accuracy: 0.8909 - val_loss: 0.1366 - val_accuracy: 0.8932\n",
      "Epoch 80/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0750 - accuracy: 0.8920 - val_loss: 0.1338 - val_accuracy: 0.9029\n",
      "Epoch 81/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0759 - accuracy: 0.8909 - val_loss: 0.1383 - val_accuracy: 0.9029\n",
      "Epoch 82/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0727 - accuracy: 0.9028 - val_loss: 0.1392 - val_accuracy: 0.8932\n",
      "Epoch 83/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0737 - accuracy: 0.8909 - val_loss: 0.1330 - val_accuracy: 0.9029\n",
      "Epoch 84/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0709 - accuracy: 0.8909 - val_loss: 0.1338 - val_accuracy: 0.8932\n",
      "Epoch 85/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0727 - accuracy: 0.8974 - val_loss: 0.1251 - val_accuracy: 0.9029\n",
      "Epoch 86/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0771 - accuracy: 0.8888 - val_loss: 0.1237 - val_accuracy: 0.8932\n",
      "Epoch 87/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0747 - accuracy: 0.8898 - val_loss: 0.1212 - val_accuracy: 0.8835\n",
      "Epoch 88/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0718 - accuracy: 0.9039 - val_loss: 0.1240 - val_accuracy: 0.8835\n",
      "Epoch 89/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0698 - accuracy: 0.8898 - val_loss: 0.1321 - val_accuracy: 0.9029\n",
      "Epoch 90/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0778 - accuracy: 0.8942 - val_loss: 0.1289 - val_accuracy: 0.9029\n",
      "Epoch 91/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0724 - accuracy: 0.8985 - val_loss: 0.1282 - val_accuracy: 0.9029\n",
      "Epoch 92/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0704 - accuracy: 0.9006 - val_loss: 0.1335 - val_accuracy: 0.9029\n",
      "Epoch 93/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0655 - accuracy: 0.8942 - val_loss: 0.1323 - val_accuracy: 0.9029\n",
      "Epoch 94/200\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.0708 - accuracy: 0.8974 - val_loss: 0.1298 - val_accuracy: 0.9029\n",
      "Epoch 95/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0694 - accuracy: 0.9017 - val_loss: 0.1343 - val_accuracy: 0.9126\n",
      "Epoch 96/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0722 - accuracy: 0.8963 - val_loss: 0.1371 - val_accuracy: 0.9029\n",
      "Epoch 97/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0737 - accuracy: 0.8996 - val_loss: 0.1309 - val_accuracy: 0.9029\n",
      "Epoch 98/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0766 - accuracy: 0.8920 - val_loss: 0.1320 - val_accuracy: 0.9029\n",
      "Epoch 99/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0709 - accuracy: 0.8942 - val_loss: 0.1397 - val_accuracy: 0.8835\n",
      "Epoch 100/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0723 - accuracy: 0.9017 - val_loss: 0.1318 - val_accuracy: 0.9029\n",
      "Epoch 101/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0749 - accuracy: 0.8866 - val_loss: 0.1224 - val_accuracy: 0.8835\n",
      "Epoch 102/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0662 - accuracy: 0.9050 - val_loss: 0.1225 - val_accuracy: 0.9029\n",
      "Epoch 103/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0724 - accuracy: 0.8942 - val_loss: 0.1189 - val_accuracy: 0.9029\n",
      "Epoch 104/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0706 - accuracy: 0.9039 - val_loss: 0.1217 - val_accuracy: 0.8932\n",
      "Epoch 105/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0715 - accuracy: 0.8942 - val_loss: 0.1206 - val_accuracy: 0.8932\n",
      "Epoch 106/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0651 - accuracy: 0.9039 - val_loss: 0.1294 - val_accuracy: 0.9029\n",
      "Epoch 107/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0700 - accuracy: 0.8888 - val_loss: 0.1327 - val_accuracy: 0.8835\n",
      "Epoch 108/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0687 - accuracy: 0.9082 - val_loss: 0.1311 - val_accuracy: 0.9029\n",
      "Epoch 109/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0597 - accuracy: 0.9060 - val_loss: 0.1286 - val_accuracy: 0.8932\n",
      "Epoch 110/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0635 - accuracy: 0.9082 - val_loss: 0.1296 - val_accuracy: 0.9029\n",
      "Epoch 111/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0650 - accuracy: 0.9028 - val_loss: 0.1276 - val_accuracy: 0.8835\n",
      "Epoch 112/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0654 - accuracy: 0.9060 - val_loss: 0.1272 - val_accuracy: 0.9029\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926/926 [==============================] - 0s 40us/step - loss: 0.0689 - accuracy: 0.9050 - val_loss: 0.1325 - val_accuracy: 0.9029\n",
      "Epoch 114/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0628 - accuracy: 0.9028 - val_loss: 0.1267 - val_accuracy: 0.9029\n",
      "Epoch 115/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0643 - accuracy: 0.9093 - val_loss: 0.1240 - val_accuracy: 0.8738\n",
      "Epoch 116/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0660 - accuracy: 0.8974 - val_loss: 0.1308 - val_accuracy: 0.8932\n",
      "Epoch 117/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0669 - accuracy: 0.9050 - val_loss: 0.1309 - val_accuracy: 0.8932\n",
      "Epoch 118/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0648 - accuracy: 0.9071 - val_loss: 0.1320 - val_accuracy: 0.9029\n",
      "Epoch 119/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0657 - accuracy: 0.9082 - val_loss: 0.1283 - val_accuracy: 0.9029\n",
      "Epoch 120/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0677 - accuracy: 0.9028 - val_loss: 0.1271 - val_accuracy: 0.9029\n",
      "Epoch 121/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0712 - accuracy: 0.8974 - val_loss: 0.1316 - val_accuracy: 0.8738\n",
      "Epoch 122/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0606 - accuracy: 0.9125 - val_loss: 0.1280 - val_accuracy: 0.8738\n",
      "Epoch 123/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0657 - accuracy: 0.9071 - val_loss: 0.1275 - val_accuracy: 0.8738\n",
      "Epoch 124/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0624 - accuracy: 0.9017 - val_loss: 0.1240 - val_accuracy: 0.8932\n",
      "Epoch 125/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0617 - accuracy: 0.9158 - val_loss: 0.1227 - val_accuracy: 0.8932\n",
      "Epoch 126/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0652 - accuracy: 0.9050 - val_loss: 0.1263 - val_accuracy: 0.8835\n",
      "Epoch 127/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0608 - accuracy: 0.9082 - val_loss: 0.1287 - val_accuracy: 0.8835\n",
      "Epoch 128/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0601 - accuracy: 0.9114 - val_loss: 0.1284 - val_accuracy: 0.8835\n",
      "Epoch 129/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0647 - accuracy: 0.9093 - val_loss: 0.1283 - val_accuracy: 0.8835\n",
      "Epoch 130/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0580 - accuracy: 0.9222 - val_loss: 0.1354 - val_accuracy: 0.8932\n",
      "Epoch 131/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0605 - accuracy: 0.9050 - val_loss: 0.1306 - val_accuracy: 0.8932\n",
      "Epoch 132/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0623 - accuracy: 0.9104 - val_loss: 0.1276 - val_accuracy: 0.8932\n",
      "Epoch 133/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0645 - accuracy: 0.9114 - val_loss: 0.1254 - val_accuracy: 0.8835\n",
      "Epoch 134/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0580 - accuracy: 0.9158 - val_loss: 0.1233 - val_accuracy: 0.8835\n",
      "Epoch 135/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0589 - accuracy: 0.9114 - val_loss: 0.1236 - val_accuracy: 0.8835\n",
      "Epoch 136/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0642 - accuracy: 0.9136 - val_loss: 0.1290 - val_accuracy: 0.8738\n",
      "Epoch 137/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0657 - accuracy: 0.9060 - val_loss: 0.1368 - val_accuracy: 0.8932\n",
      "Epoch 138/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0616 - accuracy: 0.9114 - val_loss: 0.1278 - val_accuracy: 0.8932\n",
      "Epoch 139/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0630 - accuracy: 0.9060 - val_loss: 0.1324 - val_accuracy: 0.8641\n",
      "Epoch 140/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0648 - accuracy: 0.9082 - val_loss: 0.1403 - val_accuracy: 0.8738\n",
      "Epoch 141/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0588 - accuracy: 0.9179 - val_loss: 0.1362 - val_accuracy: 0.8738\n",
      "Epoch 142/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0593 - accuracy: 0.9190 - val_loss: 0.1330 - val_accuracy: 0.8932\n",
      "Epoch 143/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0560 - accuracy: 0.9136 - val_loss: 0.1329 - val_accuracy: 0.8835\n",
      "Epoch 144/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0613 - accuracy: 0.9212 - val_loss: 0.1276 - val_accuracy: 0.8641\n",
      "Epoch 145/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0581 - accuracy: 0.9158 - val_loss: 0.1235 - val_accuracy: 0.8641\n",
      "Epoch 146/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0627 - accuracy: 0.9017 - val_loss: 0.1239 - val_accuracy: 0.8641\n",
      "Epoch 147/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0588 - accuracy: 0.9168 - val_loss: 0.1285 - val_accuracy: 0.8544\n",
      "Epoch 148/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0574 - accuracy: 0.9147 - val_loss: 0.1318 - val_accuracy: 0.8835\n",
      "Epoch 149/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0572 - accuracy: 0.9255 - val_loss: 0.1306 - val_accuracy: 0.8835\n",
      "Epoch 150/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0543 - accuracy: 0.9168 - val_loss: 0.1310 - val_accuracy: 0.8835\n",
      "Epoch 151/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0582 - accuracy: 0.9201 - val_loss: 0.1262 - val_accuracy: 0.8738\n",
      "Epoch 152/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0583 - accuracy: 0.9233 - val_loss: 0.1296 - val_accuracy: 0.8447\n",
      "Epoch 153/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0596 - accuracy: 0.9147 - val_loss: 0.1286 - val_accuracy: 0.8447\n",
      "Epoch 154/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0587 - accuracy: 0.9136 - val_loss: 0.1273 - val_accuracy: 0.8544\n",
      "Epoch 155/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0566 - accuracy: 0.9125 - val_loss: 0.1244 - val_accuracy: 0.8932\n",
      "Epoch 156/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0570 - accuracy: 0.9179 - val_loss: 0.1281 - val_accuracy: 0.8932\n",
      "Epoch 157/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0570 - accuracy: 0.9125 - val_loss: 0.1222 - val_accuracy: 0.8835\n",
      "Epoch 158/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0576 - accuracy: 0.9212 - val_loss: 0.1229 - val_accuracy: 0.8835\n",
      "Epoch 159/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0568 - accuracy: 0.9212 - val_loss: 0.1278 - val_accuracy: 0.8738\n",
      "Epoch 160/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0546 - accuracy: 0.9136 - val_loss: 0.1291 - val_accuracy: 0.8835\n",
      "Epoch 161/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0555 - accuracy: 0.9222 - val_loss: 0.1278 - val_accuracy: 0.8932\n",
      "Epoch 162/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0540 - accuracy: 0.9179 - val_loss: 0.1350 - val_accuracy: 0.8835\n",
      "Epoch 163/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0554 - accuracy: 0.9222 - val_loss: 0.1331 - val_accuracy: 0.8738\n",
      "Epoch 164/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0521 - accuracy: 0.9320 - val_loss: 0.1371 - val_accuracy: 0.8738\n",
      "Epoch 165/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0559 - accuracy: 0.9190 - val_loss: 0.1393 - val_accuracy: 0.8835\n",
      "Epoch 166/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0598 - accuracy: 0.9179 - val_loss: 0.1333 - val_accuracy: 0.8835\n",
      "Epoch 167/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0553 - accuracy: 0.9233 - val_loss: 0.1241 - val_accuracy: 0.8738\n",
      "Epoch 168/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0552 - accuracy: 0.9179 - val_loss: 0.1258 - val_accuracy: 0.8738\n",
      "Epoch 169/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0531 - accuracy: 0.9179 - val_loss: 0.1234 - val_accuracy: 0.8738\n",
      "Epoch 170/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0520 - accuracy: 0.9233 - val_loss: 0.1220 - val_accuracy: 0.8738\n",
      "Epoch 171/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0577 - accuracy: 0.9179 - val_loss: 0.1276 - val_accuracy: 0.8641\n",
      "Epoch 172/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0594 - accuracy: 0.9158 - val_loss: 0.1288 - val_accuracy: 0.8738\n",
      "Epoch 173/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0592 - accuracy: 0.9093 - val_loss: 0.1317 - val_accuracy: 0.8738\n",
      "Epoch 174/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0562 - accuracy: 0.9179 - val_loss: 0.1292 - val_accuracy: 0.8835\n",
      "Epoch 175/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0525 - accuracy: 0.9222 - val_loss: 0.1318 - val_accuracy: 0.8835\n",
      "Epoch 176/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0558 - accuracy: 0.9179 - val_loss: 0.1337 - val_accuracy: 0.8835\n",
      "Epoch 177/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0554 - accuracy: 0.9179 - val_loss: 0.1284 - val_accuracy: 0.8641\n",
      "Epoch 178/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0577 - accuracy: 0.9190 - val_loss: 0.1298 - val_accuracy: 0.8835\n",
      "Epoch 179/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0555 - accuracy: 0.9222 - val_loss: 0.1368 - val_accuracy: 0.9029\n",
      "Epoch 180/200\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0490 - accuracy: 0.9352 - val_loss: 0.1363 - val_accuracy: 0.8932\n",
      "Epoch 181/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0569 - accuracy: 0.9071 - val_loss: 0.1355 - val_accuracy: 0.8641\n",
      "Epoch 182/200\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0570 - accuracy: 0.9201 - val_loss: 0.1358 - val_accuracy: 0.8544\n",
      "Epoch 183/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0630 - accuracy: 0.9212 - val_loss: 0.1347 - val_accuracy: 0.8544\n",
      "Epoch 184/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0498 - accuracy: 0.9222 - val_loss: 0.1372 - val_accuracy: 0.8738\n",
      "Epoch 185/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0507 - accuracy: 0.9244 - val_loss: 0.1344 - val_accuracy: 0.8738\n",
      "Epoch 186/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0565 - accuracy: 0.9190 - val_loss: 0.1325 - val_accuracy: 0.8932\n",
      "Epoch 187/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0562 - accuracy: 0.9125 - val_loss: 0.1330 - val_accuracy: 0.8738\n",
      "Epoch 188/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0532 - accuracy: 0.9222 - val_loss: 0.1293 - val_accuracy: 0.8738\n",
      "Epoch 189/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0522 - accuracy: 0.9212 - val_loss: 0.1353 - val_accuracy: 0.8835\n",
      "Epoch 190/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0504 - accuracy: 0.9298 - val_loss: 0.1342 - val_accuracy: 0.8738\n",
      "Epoch 191/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0498 - accuracy: 0.9233 - val_loss: 0.1346 - val_accuracy: 0.8835\n",
      "Epoch 192/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0520 - accuracy: 0.9255 - val_loss: 0.1340 - val_accuracy: 0.8641\n",
      "Epoch 193/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0493 - accuracy: 0.9298 - val_loss: 0.1354 - val_accuracy: 0.8641\n",
      "Epoch 194/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0510 - accuracy: 0.9179 - val_loss: 0.1428 - val_accuracy: 0.8738\n",
      "Epoch 195/200\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0489 - accuracy: 0.9309 - val_loss: 0.1436 - val_accuracy: 0.8738\n",
      "Epoch 196/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0516 - accuracy: 0.9287 - val_loss: 0.1400 - val_accuracy: 0.8738\n",
      "Epoch 197/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0513 - accuracy: 0.9233 - val_loss: 0.1379 - val_accuracy: 0.8835\n",
      "Epoch 198/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0571 - accuracy: 0.9190 - val_loss: 0.1398 - val_accuracy: 0.8835\n",
      "Epoch 199/200\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0530 - accuracy: 0.9233 - val_loss: 0.1389 - val_accuracy: 0.8835\n",
      "Epoch 200/200\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0529 - accuracy: 0.9104 - val_loss: 0.1358 - val_accuracy: 0.8835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f91d4812850>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 0.8,\n",
    "                1: 0.2}\n",
    "dropout = 0.7\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(200, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "dropout = 0.7\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid', kernel_initializer= 'he_normal'))\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 200, batch_size = 30, validation_split=0.1, class_weight= class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94       863\n",
      "           1       1.00      0.39      0.56       166\n",
      "\n",
      "    accuracy                           0.90      1029\n",
      "   macro avg       0.95      0.69      0.75      1029\n",
      "weighted avg       0.91      0.90      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       370\n",
      "           1       0.94      0.21      0.34        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.90      0.60      0.64       441\n",
      "weighted avg       0.88      0.87      0.83       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.91 for x in model.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.9 for x in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.90).fit(X_train)\n",
    "pca_x_train = pca.transform(X_train)\n",
    "pca_x_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Attrition\", axis =1)\n",
    "y = df[\"Attrition\"]\n",
    "X_new = SelectKBest(chi2, k=30).fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.30, random_state=42, shuffle = True, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       863\n",
      "           1       1.00      0.01      0.02       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.92      0.51      0.47      1029\n",
      "weighted avg       0.87      0.84      0.77      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.92       370\n",
      "           1       1.00      0.04      0.08        71\n",
      "\n",
      "    accuracy                           0.85       441\n",
      "   macro avg       0.92      0.52      0.50       441\n",
      "weighted avg       0.87      0.85      0.78       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = LogisticRegression(class_weight = {0:0.85, 1:0.15}).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, svc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/50\n",
      "926/926 [==============================] - 0s 296us/step - loss: 0.3634 - accuracy: 0.8305 - val_loss: 0.4092 - val_accuracy: 0.7961\n",
      "Epoch 2/50\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.2333 - accuracy: 0.8434 - val_loss: 0.3533 - val_accuracy: 0.7961\n",
      "Epoch 3/50\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.1674 - accuracy: 0.8434 - val_loss: 0.3025 - val_accuracy: 0.7961\n",
      "Epoch 4/50\n",
      "926/926 [==============================] - 0s 53us/step - loss: 0.1340 - accuracy: 0.8434 - val_loss: 0.2595 - val_accuracy: 0.7961\n",
      "Epoch 5/50\n",
      "926/926 [==============================] - 0s 53us/step - loss: 0.1187 - accuracy: 0.8434 - val_loss: 0.2202 - val_accuracy: 0.7961\n",
      "Epoch 6/50\n",
      "926/926 [==============================] - 0s 50us/step - loss: 0.1101 - accuracy: 0.8434 - val_loss: 0.1955 - val_accuracy: 0.7961\n",
      "Epoch 7/50\n",
      "926/926 [==============================] - 0s 48us/step - loss: 0.1070 - accuracy: 0.8434 - val_loss: 0.1695 - val_accuracy: 0.7961\n",
      "Epoch 8/50\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.1045 - accuracy: 0.8434 - val_loss: 0.1552 - val_accuracy: 0.7961\n",
      "Epoch 9/50\n",
      "926/926 [==============================] - 0s 39us/step - loss: 0.1000 - accuracy: 0.8434 - val_loss: 0.1412 - val_accuracy: 0.7961\n",
      "Epoch 10/50\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.1010 - accuracy: 0.8434 - val_loss: 0.1315 - val_accuracy: 0.7961\n",
      "Epoch 11/50\n",
      "926/926 [==============================] - 0s 47us/step - loss: 0.0990 - accuracy: 0.8434 - val_loss: 0.1245 - val_accuracy: 0.7961\n",
      "Epoch 12/50\n",
      "926/926 [==============================] - 0s 109us/step - loss: 0.1004 - accuracy: 0.8434 - val_loss: 0.1218 - val_accuracy: 0.7961\n",
      "Epoch 13/50\n",
      "926/926 [==============================] - 0s 62us/step - loss: 0.0979 - accuracy: 0.8434 - val_loss: 0.1191 - val_accuracy: 0.7961\n",
      "Epoch 14/50\n",
      "926/926 [==============================] - 0s 51us/step - loss: 0.0970 - accuracy: 0.8434 - val_loss: 0.1181 - val_accuracy: 0.7961\n",
      "Epoch 15/50\n",
      "926/926 [==============================] - 0s 47us/step - loss: 0.0963 - accuracy: 0.8434 - val_loss: 0.1171 - val_accuracy: 0.7961\n",
      "Epoch 16/50\n",
      "926/926 [==============================] - 0s 53us/step - loss: 0.0962 - accuracy: 0.8434 - val_loss: 0.1165 - val_accuracy: 0.7961\n",
      "Epoch 17/50\n",
      "926/926 [==============================] - 0s 50us/step - loss: 0.0942 - accuracy: 0.8434 - val_loss: 0.1169 - val_accuracy: 0.7961\n",
      "Epoch 18/50\n",
      "926/926 [==============================] - 0s 56us/step - loss: 0.0939 - accuracy: 0.8434 - val_loss: 0.1187 - val_accuracy: 0.7961\n",
      "Epoch 19/50\n",
      "926/926 [==============================] - 0s 52us/step - loss: 0.0943 - accuracy: 0.8445 - val_loss: 0.1184 - val_accuracy: 0.7961\n",
      "Epoch 20/50\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0937 - accuracy: 0.8445 - val_loss: 0.1186 - val_accuracy: 0.7961\n",
      "Epoch 21/50\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0938 - accuracy: 0.8456 - val_loss: 0.1186 - val_accuracy: 0.7961\n",
      "Epoch 22/50\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0931 - accuracy: 0.8467 - val_loss: 0.1194 - val_accuracy: 0.7961\n",
      "Epoch 23/50\n",
      "926/926 [==============================] - 0s 37us/step - loss: 0.0924 - accuracy: 0.8456 - val_loss: 0.1185 - val_accuracy: 0.7961\n",
      "Epoch 24/50\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0913 - accuracy: 0.8456 - val_loss: 0.1190 - val_accuracy: 0.7961\n",
      "Epoch 25/50\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0915 - accuracy: 0.8456 - val_loss: 0.1196 - val_accuracy: 0.7961\n",
      "Epoch 26/50\n",
      "926/926 [==============================] - 0s 38us/step - loss: 0.0925 - accuracy: 0.8477 - val_loss: 0.1184 - val_accuracy: 0.8058\n",
      "Epoch 27/50\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0908 - accuracy: 0.8488 - val_loss: 0.1191 - val_accuracy: 0.8058\n",
      "Epoch 28/50\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0919 - accuracy: 0.8477 - val_loss: 0.1197 - val_accuracy: 0.8155\n",
      "Epoch 29/50\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0918 - accuracy: 0.8499 - val_loss: 0.1215 - val_accuracy: 0.8058\n",
      "Epoch 30/50\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0899 - accuracy: 0.8542 - val_loss: 0.1187 - val_accuracy: 0.8155\n",
      "Epoch 31/50\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0902 - accuracy: 0.8499 - val_loss: 0.1189 - val_accuracy: 0.8155\n",
      "Epoch 32/50\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0888 - accuracy: 0.8531 - val_loss: 0.1194 - val_accuracy: 0.8155\n",
      "Epoch 33/50\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0888 - accuracy: 0.8531 - val_loss: 0.1196 - val_accuracy: 0.8155\n",
      "Epoch 34/50\n",
      "926/926 [==============================] - 0s 77us/step - loss: 0.0879 - accuracy: 0.8510 - val_loss: 0.1195 - val_accuracy: 0.8155\n",
      "Epoch 35/50\n",
      "926/926 [==============================] - 0s 105us/step - loss: 0.0878 - accuracy: 0.8564 - val_loss: 0.1205 - val_accuracy: 0.8155\n",
      "Epoch 36/50\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.0871 - accuracy: 0.8542 - val_loss: 0.1188 - val_accuracy: 0.8155\n",
      "Epoch 37/50\n",
      "926/926 [==============================] - 0s 54us/step - loss: 0.0883 - accuracy: 0.8607 - val_loss: 0.1198 - val_accuracy: 0.8155\n",
      "Epoch 38/50\n",
      "926/926 [==============================] - 0s 46us/step - loss: 0.0862 - accuracy: 0.8553 - val_loss: 0.1200 - val_accuracy: 0.8155\n",
      "Epoch 39/50\n",
      "926/926 [==============================] - 0s 54us/step - loss: 0.0863 - accuracy: 0.8564 - val_loss: 0.1210 - val_accuracy: 0.8155\n",
      "Epoch 40/50\n",
      "926/926 [==============================] - 0s 50us/step - loss: 0.0858 - accuracy: 0.8618 - val_loss: 0.1204 - val_accuracy: 0.8155\n",
      "Epoch 41/50\n",
      "926/926 [==============================] - 0s 46us/step - loss: 0.0881 - accuracy: 0.8618 - val_loss: 0.1212 - val_accuracy: 0.8155\n",
      "Epoch 42/50\n",
      "926/926 [==============================] - 0s 45us/step - loss: 0.0848 - accuracy: 0.8618 - val_loss: 0.1200 - val_accuracy: 0.8155\n",
      "Epoch 43/50\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0850 - accuracy: 0.8596 - val_loss: 0.1205 - val_accuracy: 0.8155\n",
      "Epoch 44/50\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0852 - accuracy: 0.8639 - val_loss: 0.1206 - val_accuracy: 0.8155\n",
      "Epoch 45/50\n",
      "926/926 [==============================] - 0s 41us/step - loss: 0.0853 - accuracy: 0.8618 - val_loss: 0.1213 - val_accuracy: 0.8155\n",
      "Epoch 46/50\n",
      "926/926 [==============================] - 0s 49us/step - loss: 0.0828 - accuracy: 0.8661 - val_loss: 0.1206 - val_accuracy: 0.8155\n",
      "Epoch 47/50\n",
      "926/926 [==============================] - 0s 40us/step - loss: 0.0836 - accuracy: 0.8585 - val_loss: 0.1203 - val_accuracy: 0.8155\n",
      "Epoch 48/50\n",
      "926/926 [==============================] - 0s 43us/step - loss: 0.0832 - accuracy: 0.8661 - val_loss: 0.1215 - val_accuracy: 0.8155\n",
      "Epoch 49/50\n",
      "926/926 [==============================] - 0s 44us/step - loss: 0.0845 - accuracy: 0.8629 - val_loss: 0.1199 - val_accuracy: 0.8155\n",
      "Epoch 50/50\n",
      "926/926 [==============================] - 0s 42us/step - loss: 0.0821 - accuracy: 0.8737 - val_loss: 0.1205 - val_accuracy: 0.8155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f91da9759d0>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 0.8,\n",
    "                1: 0.2}\n",
    "dropout = 0.7\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, kernel_initializer= 'truncated_normal',input_shape = (X_new.shape[1],)))\n",
    "# model.add(Dropout(dropout))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid', kernel_initializer= 'truncated_normal'))\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 50, batch_size = 30, validation_split=0.1, class_weight= class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       863\n",
      "           1       1.00      0.01      0.01       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.92      0.50      0.46      1029\n",
      "weighted avg       0.87      0.84      0.77      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       1.00      0.03      0.05        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.92      0.51      0.48       441\n",
      "weighted avg       0.87      0.84      0.78       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.8 for x in model.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.8 for x in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       863\n",
      "           1       1.00      0.08      0.16       166\n",
      "\n",
      "    accuracy                           0.85      1029\n",
      "   macro avg       0.93      0.54      0.54      1029\n",
      "weighted avg       0.87      0.85      0.80      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       1.00      0.03      0.05        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.92      0.51      0.48       441\n",
      "weighted avg       0.87      0.84      0.78       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = RandomForestClassifier(class_weight = {0: 0.8,\n",
    "                1: 0.2}, n_estimators= 300, max_depth = 10, min_samples_split=3, min_samples_leaf=7).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, svc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X = df.drop(\"Attrition\", axis =1)\n",
    "y = df[\"Attrition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = RandomForestClassifier(class_weight = {0: 0.75,\n",
    "#                 1: 0.25}, n_estimators= 50, max_depth = 5, min_samples_split=5, min_samples_leaf=9).fit(X_res, y_res)\n",
    "# print('train')\n",
    "# print(classification_report(y_train, svc.predict(X_train)))\n",
    "# print('test')\n",
    "# print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = LogisticRegression(class_weight = {0:0.5, 1:0.5}).fit(X_res, y_res)\n",
    "# print('train')\n",
    "# print(classification_report(y_train, svc.predict(X_train)))\n",
    "# print('test')\n",
    "# print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"transfer.unifida.co.uk\"                    #hard-coded\n",
    "port = 22\n",
    "transport = paramiko.Transport((host, port))\n",
    "\n",
    "password = \"5tBtDzP!!s\"                #hard-coded\n",
    "username = \"DataGrasp\"                #hard-coded\n",
    "transport.connect(username = username, password = password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "sftp = paramiko.SFTPClient.from_transport(transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = pd.read_csv(file_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "second = pd.read_csv(file_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURN</th>\n",
       "      <th>OrderID</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>OrderValue</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>ReferrerType</th>\n",
       "      <th>ReferrerName</th>\n",
       "      <th>referrerUrl</th>\n",
       "      <th>RecencyDays</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>DeviceBrand</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceModel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356543</th>\n",
       "      <td>955298</td>\n",
       "      <td>424936</td>\n",
       "      <td>2019-12-23 23:59:00.000</td>\n",
       "      <td>291.4100</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2019-12-01 12:30:30.000</td>\n",
       "      <td>Direct Entry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>Hwaseong-si</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>GALAXY A5 (2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356544</th>\n",
       "      <td>119842</td>\n",
       "      <td>384861</td>\n",
       "      <td>2019-06-19 23:59:00.000</td>\n",
       "      <td>27.1584</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2018-11-09 12:29:56.000</td>\n",
       "      <td>Campaigns</td>\n",
       "      <td>great gifts for grandmas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Generic Desktop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356545</th>\n",
       "      <td>120076</td>\n",
       "      <td>389733</td>\n",
       "      <td>2019-08-23 23:59:00.000</td>\n",
       "      <td>88.4418</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2019-06-21 09:15:47.000</td>\n",
       "      <td>Direct Entry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>Bridgwater</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356546</th>\n",
       "      <td>120188</td>\n",
       "      <td>358802</td>\n",
       "      <td>2018-11-25 23:59:00.000</td>\n",
       "      <td>48.7500</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2018-10-29 20:03:55.000</td>\n",
       "      <td>Direct Entry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356547</th>\n",
       "      <td>120217</td>\n",
       "      <td>448479</td>\n",
       "      <td>2020-04-16 23:59:00.000</td>\n",
       "      <td>49.1240</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2019-11-30 08:52:43.000</td>\n",
       "      <td>Campaigns</td>\n",
       "      <td>black friday 1 2019 uk</td>\n",
       "      <td>android-app://com.google.android.gm</td>\n",
       "      <td>138</td>\n",
       "      <td>London</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>Generic Smartphone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PURN  OrderID          TransactionDate  OrderValue CustomerType  \\\n",
       "356543  955298   424936  2019-12-23 23:59:00.000    291.4100     Existing   \n",
       "356544  119842   384861  2019-06-19 23:59:00.000     27.1584     Existing   \n",
       "356545  120076   389733  2019-08-23 23:59:00.000     88.4418     Existing   \n",
       "356546  120188   358802  2018-11-25 23:59:00.000     48.7500     Existing   \n",
       "356547  120217   448479  2020-04-16 23:59:00.000     49.1240     Existing   \n",
       "\n",
       "                      EventDate  ReferrerType              ReferrerName  \\\n",
       "356543  2019-12-01 12:30:30.000  Direct Entry                       NaN   \n",
       "356544  2018-11-09 12:29:56.000     Campaigns  great gifts for grandmas   \n",
       "356545  2019-06-21 09:15:47.000  Direct Entry                       NaN   \n",
       "356546  2018-10-29 20:03:55.000  Direct Entry                       NaN   \n",
       "356547  2019-11-30 08:52:43.000     Campaigns    black friday 1 2019 uk   \n",
       "\n",
       "                                referrerUrl  RecencyDays         City  \\\n",
       "356543                                  NaN           22  Hwaseong-si   \n",
       "356544                                  NaN          222          NaN   \n",
       "356545                                  NaN           63   Bridgwater   \n",
       "356546                                  NaN           27       London   \n",
       "356547  android-app://com.google.android.gm          138       London   \n",
       "\n",
       "               Country Continent DeviceBrand  DeviceType         DeviceModel  \n",
       "356543     South Korea      Asia     Samsung  Smartphone    GALAXY A5 (2017)  \n",
       "356544  United Kingdom    Europe     Unknown     Desktop     Generic Desktop  \n",
       "356545  United Kingdom    Europe       Apple      Tablet                iPad  \n",
       "356546  United Kingdom    Europe       Apple  Smartphone              iPhone  \n",
       "356547  United Kingdom    Europe     Unknown  Smartphone  Generic Smartphone  "
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURN</th>\n",
       "      <th>OrderID</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>OrderValue</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>ReferrerType</th>\n",
       "      <th>ReferrerName</th>\n",
       "      <th>referrerUrl</th>\n",
       "      <th>RecencyDays</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>DeviceBrand</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceModel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250182</td>\n",
       "      <td>1395213</td>\n",
       "      <td>2020-05-11 23:59:00.000</td>\n",
       "      <td>733.3333</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2020-05-10 18:46:01.000</td>\n",
       "      <td>Websites</td>\n",
       "      <td>webservices.global-e.com</td>\n",
       "      <td>https://www.johnstonsofelgin.com/us/checkout/c...</td>\n",
       "      <td>1</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250236</td>\n",
       "      <td>1395163</td>\n",
       "      <td>2020-05-10 23:59:00.000</td>\n",
       "      <td>550.0700</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2020-05-10 10:37:41.000</td>\n",
       "      <td>Direct Entry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.johnstonsofelgin.com/retail/checko...</td>\n",
       "      <td>0</td>\n",
       "      <td>Edinburgh</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Generic Desktop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21129</td>\n",
       "      <td>1395368</td>\n",
       "      <td>2020-05-12 23:59:00.000</td>\n",
       "      <td>165.8300</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2020-05-11 09:00:54.000</td>\n",
       "      <td>Campaigns</td>\n",
       "      <td>2020_01_mss_09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Etchingham</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Generic Desktop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21129</td>\n",
       "      <td>1395936</td>\n",
       "      <td>2020-06-03 23:59:00.000</td>\n",
       "      <td>74.1600</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2020-05-11 09:00:54.000</td>\n",
       "      <td>Campaigns</td>\n",
       "      <td>2020_01_mss_09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>Etchingham</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Generic Desktop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250779</td>\n",
       "      <td>1395638</td>\n",
       "      <td>2020-05-22 23:59:00.000</td>\n",
       "      <td>344.0800</td>\n",
       "      <td>Existing</td>\n",
       "      <td>2020-05-10 15:54:29.000</td>\n",
       "      <td>Direct Entry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>Prestonpans</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Smartphone</td>\n",
       "      <td>iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PURN  OrderID          TransactionDate  OrderValue CustomerType  \\\n",
       "0  250182  1395213  2020-05-11 23:59:00.000    733.3333     Existing   \n",
       "1  250236  1395163  2020-05-10 23:59:00.000    550.0700     Existing   \n",
       "2   21129  1395368  2020-05-12 23:59:00.000    165.8300     Existing   \n",
       "3   21129  1395936  2020-06-03 23:59:00.000     74.1600     Existing   \n",
       "4  250779  1395638  2020-05-22 23:59:00.000    344.0800     Existing   \n",
       "\n",
       "                 EventDate  ReferrerType              ReferrerName  \\\n",
       "0  2020-05-10 18:46:01.000      Websites  webservices.global-e.com   \n",
       "1  2020-05-10 10:37:41.000  Direct Entry                       NaN   \n",
       "2  2020-05-11 09:00:54.000     Campaigns            2020_01_mss_09   \n",
       "3  2020-05-11 09:00:54.000     Campaigns            2020_01_mss_09   \n",
       "4  2020-05-10 15:54:29.000  Direct Entry                       NaN   \n",
       "\n",
       "                                         referrerUrl  RecencyDays  \\\n",
       "0  https://www.johnstonsofelgin.com/us/checkout/c...            1   \n",
       "1  https://www.johnstonsofelgin.com/retail/checko...            0   \n",
       "2                                                NaN            1   \n",
       "3                                                NaN           23   \n",
       "4                                                NaN           12   \n",
       "\n",
       "          City         Country      Continent DeviceBrand  DeviceType  \\\n",
       "0    Charlotte   United States  North America       Apple  Smartphone   \n",
       "1    Edinburgh  United Kingdom         Europe       Apple     Desktop   \n",
       "2   Etchingham  United Kingdom         Europe     Unknown     Desktop   \n",
       "3   Etchingham  United Kingdom         Europe     Unknown     Desktop   \n",
       "4  Prestonpans  United Kingdom         Europe       Apple  Smartphone   \n",
       "\n",
       "      DeviceModel   \n",
       "0           iPhone  \n",
       "1  Generic Desktop  \n",
       "2  Generic Desktop  \n",
       "3  Generic Desktop  \n",
       "4           iPhone  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23:59:00.000'"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first['TransactionDate'][0].split(\" \")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
