{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "###The following columns have no information provided, and they seems to be incosistent with what \n",
    "## some of the data columns in the columns already have\n",
    "for x in data.columns:\n",
    "    if x[-4:] == \"Rate\":\n",
    "        data = data.drop(x, axis =1)\n",
    "###The following data does not provide any relevance to the data as they are either \n",
    "###Â all have the same number, or the value does not provide any information\n",
    "data = data.drop([\"EmployeeCount\", \"EmployeeNumber\", \"StandardHours\", \"Over18\", \"EducationField\", 'Education', 'JobLevel'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"Yes\": 1, \"No\":0}\n",
    "data['StockOptionLevel'] = np.where(data['StockOptionLevel'] >0, 1,0)\n",
    "data['OverTime'] = data['OverTime'].map(mapping)\n",
    "data['Gender'] = np.where(data[\"Gender\"] == \"Male\", 1,0)\n",
    "data['BusinessTravel'] = np.where(data['BusinessTravel'] == 'Travel_Frequently', 1,0)\n",
    "data['MaritalStatus'] = np.where(data['MaritalStatus'] == 'Single',0,1)\n",
    "data['TotalJobSatisfcation'] = data['EnvironmentSatisfaction'] + data['JobSatisfaction'] + data['RelationshipSatisfaction']\n",
    "data = data.drop(['EnvironmentSatisfaction', 'JobSatisfaction', 'RelationshipSatisfaction', 'PerformanceRating'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_to_salary = {}\n",
    "for x in data['JobRole'].unique():\n",
    "    subset = data[data['JobRole'] == x]\n",
    "    role_to_salary[x] = np.mean(subset['MonthlyIncome'])\n",
    "\n",
    "data['JobRole'] = data['JobRole'].map(role_to_salary)\n",
    "\n",
    "\n",
    "department_to_salary = {}\n",
    "for x in data['Department'].unique():\n",
    "    subset = data[data['Department'] == x]\n",
    "    department_to_salary[x] = np.mean(subset['MonthlyIncome'])\n",
    "\n",
    "data['Department'] = data['Department'].map(department_to_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical = list(data.drop(['Attrition'], axis =1).describe(include = ['O']).columns)\n",
    "numerical = list(data.describe().columns)\n",
    "x = data[numerical].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "df.columns = numerical\n",
    "# for x in categorical:\n",
    "#     df[x] = data[x]\n",
    "# df = pd.get_dummies(df, columns = categorical)\n",
    "mapping = {\"Yes\": 1, \"No\":0}\n",
    "df[\"Attrition\"] = data[\"Attrition\"].map(mapping)\n",
    "X = df.drop(\"Attrition\", axis =1)\n",
    "y = df[\"Attrition\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True, stratify = y)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42, shuffle = True, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       863\n",
      "           1       0.00      0.00      0.00       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.42      0.50      0.46      1029\n",
      "weighted avg       0.70      0.84      0.77      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       0.00      0.00      0.00        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.42      0.50      0.46       441\n",
      "weighted avg       0.70      0.84      0.77       441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, dummy_clf.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, dummy_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       863\n",
      "           1       0.88      0.40      0.55       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.89      0.69      0.74      1029\n",
      "weighted avg       0.89      0.89      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       370\n",
      "           1       0.83      0.42      0.56        71\n",
      "\n",
      "    accuracy                           0.89       441\n",
      "   macro avg       0.87      0.70      0.75       441\n",
      "weighted avg       0.89      0.89      0.88       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel=\"linear\", C =1, probability= True).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, svc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(svc.predict(X_test) ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', -1.1702826940198803),\n",
       " ('BusinessTravel', 0.6956480875197286),\n",
       " ('Department', 0.5457153870831988),\n",
       " ('DistanceFromHome', 0.674852644171164),\n",
       " ('Gender', 0.1288764888692624),\n",
       " ('JobInvolvement', -0.858109687750372),\n",
       " ('JobRole', -0.9689650948879223),\n",
       " ('MaritalStatus', -0.09619114111732685),\n",
       " ('MonthlyIncome', 0.01265262733752437),\n",
       " ('NumCompaniesWorked', 1.0273659479738066),\n",
       " ('OverTime', 1.041431318501914),\n",
       " ('PercentSalaryHike', 0.1005654949384096),\n",
       " ('StockOptionLevel', -0.7842926573758184),\n",
       " ('TotalWorkingYears', -1.2313242874890167),\n",
       " ('TrainingTimesLastYear', -0.4892138701975559),\n",
       " ('WorkLifeBalance', -0.5686634812869675),\n",
       " ('YearsAtCompany', -0.40124367618727885),\n",
       " ('YearsInCurrentRole', -1.2022445678776261),\n",
       " ('YearsSinceLastPromotion', 1.707096049398664),\n",
       " ('YearsWithCurrManager', -0.7036201180812487),\n",
       " ('TotalJobSatisfcation', -1.955425340067796)]"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(list(X_train.columns), svc.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       863\n",
      "           1       0.85      0.41      0.55       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.87      0.70      0.75      1029\n",
      "weighted avg       0.89      0.89      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       370\n",
      "           1       0.73      0.38      0.50        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.81      0.68      0.72       441\n",
      "weighted avg       0.87      0.88      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lr.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       863\n",
      "           1       1.00      0.01      0.01       166\n",
      "\n",
      "    accuracy                           0.84      1029\n",
      "   macro avg       0.92      0.50      0.46      1029\n",
      "weighted avg       0.87      0.84      0.77      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       370\n",
      "           1       1.00      0.03      0.05        71\n",
      "\n",
      "    accuracy                           0.84       441\n",
      "   macro avg       0.92      0.51      0.48       441\n",
      "weighted avg       0.87      0.84      0.78       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lra = LogisticRegression(class_weight = {0:.85, 1:0.15}).fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, lra.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, lra.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([439,   2]))"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lra.predict(X_test), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([370,  71]))"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       863\n",
      "           1       0.94      0.99      0.96       166\n",
      "\n",
      "    accuracy                           0.99      1029\n",
      "   macro avg       0.97      0.99      0.98      1029\n",
      "weighted avg       0.99      0.99      0.99      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       370\n",
      "           1       0.43      0.44      0.43        71\n",
      "\n",
      "    accuracy                           0.82       441\n",
      "   macro avg       0.66      0.66      0.66       441\n",
      "weighted avg       0.82      0.82      0.82       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth =3, n_estimators = 100, scale_pos_weight = 4)\n",
    "clf.fit(X_train,y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, clf.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[\n",
    "    ('svc', SVC(kernel=\"linear\", C =1, probability= True, class_weight = {0:0.85, 1:0.15})),\n",
    "    ('clf', xgb.XGBClassifier(max_depth =1, n_estimators = 100, scale_pos_weight = 4)),\n",
    "    ('lr', LogisticRegression(class_weight = {0:0.85, 1:0.15}))], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       863\n",
      "           1       1.00      0.20      0.33       166\n",
      "\n",
      "    accuracy                           0.87      1029\n",
      "   macro avg       0.93      0.60      0.63      1029\n",
      "weighted avg       0.89      0.87      0.83      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       370\n",
      "           1       0.87      0.18      0.30        71\n",
      "\n",
      "    accuracy                           0.86       441\n",
      "   macro avg       0.87      0.59      0.61       441\n",
      "weighted avg       0.86      0.86      0.82       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vc.fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, vc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[\n",
    "    ('svc', SVC(kernel=\"linear\", C =1, probability= True, class_weight = 'balanced')),\n",
    "    ('clf', xgb.XGBClassifier(max_depth =1, n_estimators = 100)),\n",
    "    ('lr', LogisticRegression(class_weight = 'balanced'))], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       863\n",
      "           1       0.79      0.57      0.66       166\n",
      "\n",
      "    accuracy                           0.91      1029\n",
      "   macro avg       0.86      0.77      0.81      1029\n",
      "weighted avg       0.90      0.91      0.90      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93       370\n",
      "           1       0.64      0.45      0.53        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.77      0.70      0.73       441\n",
      "weighted avg       0.86      0.87      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vc.fit(X_train, y_train)\n",
    "print('train')\n",
    "print(classification_report(y_train, vc.predict(X_train)))\n",
    "print('test')\n",
    "print(classification_report(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 1s 1ms/step - loss: 0.7344 - precision_53: 0.1573 - val_loss: 0.5554 - val_precision_53: 0.1532\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 171us/step - loss: 0.4941 - precision_53: 0.1702 - val_loss: 0.5130 - val_precision_53: 0.1780\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.4864 - precision_53: 0.1824 - val_loss: 0.4814 - val_precision_53: 0.1863\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.4934 - precision_53: 0.1868 - val_loss: 0.4670 - val_precision_53: 0.1874\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 150us/step - loss: 0.4798 - precision_53: 0.1869 - val_loss: 0.4520 - val_precision_53: 0.1859\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.4458 - precision_53: 0.1874 - val_loss: 0.4430 - val_precision_53: 0.1930\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.4746 - precision_53: 0.1994 - val_loss: 0.4353 - val_precision_53: 0.2036\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.4373 - precision_53: 0.2101 - val_loss: 0.4281 - val_precision_53: 0.2130\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 149us/step - loss: 0.4453 - precision_53: 0.2150 - val_loss: 0.4186 - val_precision_53: 0.2220\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 158us/step - loss: 0.4239 - precision_53: 0.2309 - val_loss: 0.4043 - val_precision_53: 0.2384\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 143us/step - loss: 0.4008 - precision_53: 0.2417 - val_loss: 0.3935 - val_precision_53: 0.2482\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.4408 - precision_53: 0.2542 - val_loss: 0.3900 - val_precision_53: 0.2573\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.4268 - precision_53: 0.2595 - val_loss: 0.3935 - val_precision_53: 0.2662\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.4041 - precision_53: 0.2722 - val_loss: 0.3828 - val_precision_53: 0.2767\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.3937 - precision_53: 0.2827 - val_loss: 0.3779 - val_precision_53: 0.2889\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.4052 - precision_53: 0.2932 - val_loss: 0.3746 - val_precision_53: 0.2971\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.4052 - precision_53: 0.3010 - val_loss: 0.3751 - val_precision_53: 0.3072\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.3707 - precision_53: 0.3136 - val_loss: 0.3639 - val_precision_53: 0.3201\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.3798 - precision_53: 0.3259 - val_loss: 0.3572 - val_precision_53: 0.3313\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.3515 - precision_53: 0.3361 - val_loss: 0.3465 - val_precision_53: 0.3433\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.3585 - precision_53: 0.3511 - val_loss: 0.3486 - val_precision_53: 0.3569\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.3697 - precision_53: 0.3615 - val_loss: 0.3574 - val_precision_53: 0.3656\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.3632 - precision_53: 0.3709 - val_loss: 0.3514 - val_precision_53: 0.3752\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 145us/step - loss: 0.3462 - precision_53: 0.3813 - val_loss: 0.3492 - val_precision_53: 0.3870\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.3724 - precision_53: 0.3925 - val_loss: 0.3499 - val_precision_53: 0.3953\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.3512 - precision_53: 0.3982 - val_loss: 0.3506 - val_precision_53: 0.4006\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.3749 - precision_53: 0.4022 - val_loss: 0.3442 - val_precision_53: 0.4048\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.3546 - precision_53: 0.4095 - val_loss: 0.3475 - val_precision_53: 0.4138\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.3483 - precision_53: 0.4163 - val_loss: 0.3438 - val_precision_53: 0.4199\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.3406 - precision_53: 0.4241 - val_loss: 0.3429 - val_precision_53: 0.4303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f86b14e20a0>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0:0.85}\n",
    "dropout = 0.4\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "model.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       863\n",
      "           1       0.85      0.37      0.51       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.87      0.68      0.72      1029\n",
      "weighted avg       0.88      0.89      0.87      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.93       370\n",
      "           1       0.70      0.32      0.44        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.79      0.65      0.68       441\n",
      "weighted avg       0.85      0.87      0.85       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('train')\n",
    "# print(classification_report(y_train, model.predict(X_train)))\n",
    "# print('test')\n",
    "# print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in model.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 1s 1ms/step - loss: 0.3466 - precision_57: 0.1486 - val_loss: 0.3256 - val_precision_57: 0.1582\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 173us/step - loss: 0.3142 - precision_57: 0.1750 - val_loss: 0.3024 - val_precision_57: 0.1724\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.3018 - precision_57: 0.1800 - val_loss: 0.2938 - val_precision_57: 0.1827\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2987 - precision_57: 0.1876 - val_loss: 0.2898 - val_precision_57: 0.1883\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2744 - precision_57: 0.1958 - val_loss: 0.2821 - val_precision_57: 0.1982\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2877 - precision_57: 0.1974 - val_loss: 0.2777 - val_precision_57: 0.1981\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2754 - precision_57: 0.2019 - val_loss: 0.2736 - val_precision_57: 0.2031\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.2736 - precision_57: 0.2081 - val_loss: 0.2640 - val_precision_57: 0.2142\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.2693 - precision_57: 0.2221 - val_loss: 0.2578 - val_precision_57: 0.2260\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2570 - precision_57: 0.2325 - val_loss: 0.2554 - val_precision_57: 0.2420\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.2490 - precision_57: 0.2473 - val_loss: 0.2464 - val_precision_57: 0.2560\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.2434 - precision_57: 0.2639 - val_loss: 0.2423 - val_precision_57: 0.2718\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 158us/step - loss: 0.2366 - precision_57: 0.2842 - val_loss: 0.2330 - val_precision_57: 0.2998\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.2455 - precision_57: 0.3085 - val_loss: 0.2257 - val_precision_57: 0.3163\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.2365 - precision_57: 0.3238 - val_loss: 0.2223 - val_precision_57: 0.3334\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.2296 - precision_57: 0.3390 - val_loss: 0.2214 - val_precision_57: 0.3457\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.2433 - precision_57: 0.3520 - val_loss: 0.2232 - val_precision_57: 0.3582\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.2327 - precision_57: 0.3637 - val_loss: 0.2185 - val_precision_57: 0.3698\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.2291 - precision_57: 0.3744 - val_loss: 0.2209 - val_precision_57: 0.3807\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.2360 - precision_57: 0.3841 - val_loss: 0.2213 - val_precision_57: 0.3885\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2365 - precision_57: 0.3895 - val_loss: 0.2168 - val_precision_57: 0.3935\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.2211 - precision_57: 0.3976 - val_loss: 0.2091 - val_precision_57: 0.4035\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.2293 - precision_57: 0.4076 - val_loss: 0.2090 - val_precision_57: 0.4110\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 143us/step - loss: 0.2203 - precision_57: 0.4163 - val_loss: 0.2063 - val_precision_57: 0.4215\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 154us/step - loss: 0.2163 - precision_57: 0.4250 - val_loss: 0.1982 - val_precision_57: 0.4294\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 149us/step - loss: 0.2122 - precision_57: 0.4332 - val_loss: 0.1993 - val_precision_57: 0.4368\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 153us/step - loss: 0.2171 - precision_57: 0.4406 - val_loss: 0.1964 - val_precision_57: 0.4438\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2130 - precision_57: 0.4464 - val_loss: 0.1963 - val_precision_57: 0.4501\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.2078 - precision_57: 0.4521 - val_loss: 0.1986 - val_precision_57: 0.4558\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.2157 - precision_57: 0.4588 - val_loss: 0.1931 - val_precision_57: 0.4622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8691403d60>"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0:0.5, 1:0.7}\n",
    "dropout = 0.4\n",
    "modelb = keras.models.Sequential()\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelb.add(keras.layers.BatchNormalization())\n",
    "modelb.add(keras.layers.Activation('relu'))\n",
    "modelb.add(Dropout(dropout))\n",
    "modelb.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modelb.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modelb.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       863\n",
      "           1       0.78      0.44      0.56       166\n",
      "\n",
      "    accuracy                           0.89      1029\n",
      "   macro avg       0.84      0.71      0.75      1029\n",
      "weighted avg       0.88      0.89      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       370\n",
      "           1       0.63      0.41      0.50        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.76      0.68      0.71       441\n",
      "weighted avg       0.85      0.87      0.85       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modelb.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modelb.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "926/926 [==============================] - 1s 1ms/step - loss: 0.6110 - precision_58: 0.1487 - val_loss: 0.5465 - val_precision_58: 0.1542\n",
      "Epoch 2/100\n",
      "926/926 [==============================] - 0s 176us/step - loss: 0.4683 - precision_58: 0.1700 - val_loss: 0.4945 - val_precision_58: 0.1847\n",
      "Epoch 3/100\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.4484 - precision_58: 0.1996 - val_loss: 0.4639 - val_precision_58: 0.2028\n",
      "Epoch 4/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.4396 - precision_58: 0.2144 - val_loss: 0.4408 - val_precision_58: 0.2268\n",
      "Epoch 5/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.4503 - precision_58: 0.2290 - val_loss: 0.4240 - val_precision_58: 0.2318\n",
      "Epoch 6/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.4277 - precision_58: 0.2424 - val_loss: 0.4060 - val_precision_58: 0.2481\n",
      "Epoch 7/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.4108 - precision_58: 0.2598 - val_loss: 0.3943 - val_precision_58: 0.2748\n",
      "Epoch 8/100\n",
      "926/926 [==============================] - 0s 145us/step - loss: 0.4050 - precision_58: 0.2857 - val_loss: 0.3764 - val_precision_58: 0.3020\n",
      "Epoch 9/100\n",
      "926/926 [==============================] - 0s 154us/step - loss: 0.4017 - precision_58: 0.3137 - val_loss: 0.3716 - val_precision_58: 0.3315\n",
      "Epoch 10/100\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.4001 - precision_58: 0.3430 - val_loss: 0.3648 - val_precision_58: 0.3536\n",
      "Epoch 11/100\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.3789 - precision_58: 0.3657 - val_loss: 0.3531 - val_precision_58: 0.3801\n",
      "Epoch 12/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3643 - precision_58: 0.3886 - val_loss: 0.3514 - val_precision_58: 0.3993\n",
      "Epoch 13/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.3577 - precision_58: 0.4056 - val_loss: 0.3573 - val_precision_58: 0.4208\n",
      "Epoch 14/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.3871 - precision_58: 0.4249 - val_loss: 0.3638 - val_precision_58: 0.4293\n",
      "Epoch 15/100\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.3811 - precision_58: 0.4349 - val_loss: 0.3541 - val_precision_58: 0.4400\n",
      "Epoch 16/100\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.3694 - precision_58: 0.4480 - val_loss: 0.3525 - val_precision_58: 0.4571\n",
      "Epoch 17/100\n",
      "926/926 [==============================] - 0s 144us/step - loss: 0.3506 - precision_58: 0.4641 - val_loss: 0.3514 - val_precision_58: 0.4703\n",
      "Epoch 18/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3698 - precision_58: 0.4751 - val_loss: 0.3497 - val_precision_58: 0.4771\n",
      "Epoch 19/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3813 - precision_58: 0.4776 - val_loss: 0.3438 - val_precision_58: 0.4805\n",
      "Epoch 20/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.3568 - precision_58: 0.4843 - val_loss: 0.3462 - val_precision_58: 0.4902\n",
      "Epoch 21/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.3331 - precision_58: 0.4951 - val_loss: 0.3400 - val_precision_58: 0.5011\n",
      "Epoch 22/100\n",
      "926/926 [==============================] - 0s 143us/step - loss: 0.3380 - precision_58: 0.5037 - val_loss: 0.3288 - val_precision_58: 0.5084\n",
      "Epoch 23/100\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.3467 - precision_58: 0.5118 - val_loss: 0.3344 - val_precision_58: 0.5144\n",
      "Epoch 24/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.3340 - precision_58: 0.5176 - val_loss: 0.3306 - val_precision_58: 0.5198\n",
      "Epoch 25/100\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.3436 - precision_58: 0.5239 - val_loss: 0.3402 - val_precision_58: 0.5249\n",
      "Epoch 26/100\n",
      "926/926 [==============================] - 0s 152us/step - loss: 0.3291 - precision_58: 0.5285 - val_loss: 0.3386 - val_precision_58: 0.5310\n",
      "Epoch 27/100\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.3326 - precision_58: 0.5333 - val_loss: 0.3354 - val_precision_58: 0.5361\n",
      "Epoch 28/100\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.3387 - precision_58: 0.5385 - val_loss: 0.3355 - val_precision_58: 0.5418\n",
      "Epoch 29/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.3172 - precision_58: 0.5430 - val_loss: 0.3266 - val_precision_58: 0.5464\n",
      "Epoch 30/100\n",
      "926/926 [==============================] - 0s 149us/step - loss: 0.3238 - precision_58: 0.5478 - val_loss: 0.3303 - val_precision_58: 0.5509\n",
      "Epoch 31/100\n",
      "926/926 [==============================] - 0s 153us/step - loss: 0.3298 - precision_58: 0.5526 - val_loss: 0.3303 - val_precision_58: 0.5549\n",
      "Epoch 32/100\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.3216 - precision_58: 0.5577 - val_loss: 0.3349 - val_precision_58: 0.5592\n",
      "Epoch 33/100\n",
      "926/926 [==============================] - 0s 152us/step - loss: 0.3212 - precision_58: 0.5609 - val_loss: 0.3380 - val_precision_58: 0.5625\n",
      "Epoch 34/100\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.3107 - precision_58: 0.5651 - val_loss: 0.3370 - val_precision_58: 0.5668\n",
      "Epoch 35/100\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.3215 - precision_58: 0.5672 - val_loss: 0.3369 - val_precision_58: 0.5690\n",
      "Epoch 36/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.3195 - precision_58: 0.5705 - val_loss: 0.3403 - val_precision_58: 0.5709\n",
      "Epoch 37/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.3162 - precision_58: 0.5732 - val_loss: 0.3429 - val_precision_58: 0.5750\n",
      "Epoch 38/100\n",
      "926/926 [==============================] - 0s 155us/step - loss: 0.3123 - precision_58: 0.5763 - val_loss: 0.3390 - val_precision_58: 0.5776\n",
      "Epoch 39/100\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.3078 - precision_58: 0.5794 - val_loss: 0.3338 - val_precision_58: 0.5800\n",
      "Epoch 40/100\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.3023 - precision_58: 0.5815 - val_loss: 0.3358 - val_precision_58: 0.5829\n",
      "Epoch 41/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.3000 - precision_58: 0.5842 - val_loss: 0.3363 - val_precision_58: 0.5855\n",
      "Epoch 42/100\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.3095 - precision_58: 0.5868 - val_loss: 0.3389 - val_precision_58: 0.5882\n",
      "Epoch 43/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.2832 - precision_58: 0.5898 - val_loss: 0.3334 - val_precision_58: 0.5918\n",
      "Epoch 44/100\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.3071 - precision_58: 0.5923 - val_loss: 0.3396 - val_precision_58: 0.5930\n",
      "Epoch 45/100\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.3089 - precision_58: 0.5932 - val_loss: 0.3510 - val_precision_58: 0.5945\n",
      "Epoch 46/100\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.3211 - precision_58: 0.5948 - val_loss: 0.3491 - val_precision_58: 0.5948\n",
      "Epoch 47/100\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.2981 - precision_58: 0.5960 - val_loss: 0.3429 - val_precision_58: 0.5960\n",
      "Epoch 48/100\n",
      "926/926 [==============================] - 0s 143us/step - loss: 0.2861 - precision_58: 0.5969 - val_loss: 0.3435 - val_precision_58: 0.5985\n",
      "Epoch 49/100\n",
      "926/926 [==============================] - 0s 132us/step - loss: 0.3142 - precision_58: 0.5999 - val_loss: 0.3419 - val_precision_58: 0.6011\n",
      "Epoch 50/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2973 - precision_58: 0.6013 - val_loss: 0.3424 - val_precision_58: 0.6026\n",
      "Epoch 51/100\n",
      "926/926 [==============================] - 0s 139us/step - loss: 0.2926 - precision_58: 0.6045 - val_loss: 0.3482 - val_precision_58: 0.6055\n",
      "Epoch 52/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2735 - precision_58: 0.6069 - val_loss: 0.3599 - val_precision_58: 0.6080\n",
      "Epoch 53/100\n",
      "926/926 [==============================] - 0s 145us/step - loss: 0.3038 - precision_58: 0.6083 - val_loss: 0.3548 - val_precision_58: 0.6091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.3063 - precision_58: 0.6090 - val_loss: 0.3458 - val_precision_58: 0.6097\n",
      "Epoch 55/100\n",
      "926/926 [==============================] - 0s 146us/step - loss: 0.2794 - precision_58: 0.6106 - val_loss: 0.3456 - val_precision_58: 0.6122\n",
      "Epoch 56/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2878 - precision_58: 0.6135 - val_loss: 0.3347 - val_precision_58: 0.6140\n",
      "Epoch 57/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.2881 - precision_58: 0.6141 - val_loss: 0.3386 - val_precision_58: 0.6145\n",
      "Epoch 58/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2855 - precision_58: 0.6151 - val_loss: 0.3361 - val_precision_58: 0.6162\n",
      "Epoch 59/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2967 - precision_58: 0.6161 - val_loss: 0.3504 - val_precision_58: 0.6161\n",
      "Epoch 60/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2922 - precision_58: 0.6169 - val_loss: 0.3445 - val_precision_58: 0.6179\n",
      "Epoch 61/100\n",
      "926/926 [==============================] - 0s 131us/step - loss: 0.2794 - precision_58: 0.6187 - val_loss: 0.3430 - val_precision_58: 0.6203\n",
      "Epoch 62/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.2774 - precision_58: 0.6216 - val_loss: 0.3405 - val_precision_58: 0.6226\n",
      "Epoch 63/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2824 - precision_58: 0.6237 - val_loss: 0.3521 - val_precision_58: 0.6245\n",
      "Epoch 64/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.3022 - precision_58: 0.6249 - val_loss: 0.3641 - val_precision_58: 0.6254\n",
      "Epoch 65/100\n",
      "926/926 [==============================] - 0s 137us/step - loss: 0.2834 - precision_58: 0.6263 - val_loss: 0.3498 - val_precision_58: 0.6265\n",
      "Epoch 66/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2947 - precision_58: 0.6272 - val_loss: 0.3356 - val_precision_58: 0.6275\n",
      "Epoch 67/100\n",
      "926/926 [==============================] - 0s 126us/step - loss: 0.2833 - precision_58: 0.6279 - val_loss: 0.3403 - val_precision_58: 0.6280\n",
      "Epoch 68/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.2910 - precision_58: 0.6287 - val_loss: 0.3450 - val_precision_58: 0.6288\n",
      "Epoch 69/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2794 - precision_58: 0.6300 - val_loss: 0.3477 - val_precision_58: 0.6310\n",
      "Epoch 70/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2987 - precision_58: 0.6319 - val_loss: 0.3558 - val_precision_58: 0.6322\n",
      "Epoch 71/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2908 - precision_58: 0.6324 - val_loss: 0.3614 - val_precision_58: 0.6329\n",
      "Epoch 72/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2860 - precision_58: 0.6339 - val_loss: 0.3568 - val_precision_58: 0.6348\n",
      "Epoch 73/100\n",
      "926/926 [==============================] - 0s 130us/step - loss: 0.2827 - precision_58: 0.6346 - val_loss: 0.3598 - val_precision_58: 0.6350\n",
      "Epoch 74/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2751 - precision_58: 0.6355 - val_loss: 0.3476 - val_precision_58: 0.6366\n",
      "Epoch 75/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2962 - precision_58: 0.6365 - val_loss: 0.3555 - val_precision_58: 0.6365\n",
      "Epoch 76/100\n",
      "926/926 [==============================] - 0s 142us/step - loss: 0.2773 - precision_58: 0.6374 - val_loss: 0.3729 - val_precision_58: 0.6376\n",
      "Epoch 77/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2734 - precision_58: 0.6380 - val_loss: 0.3730 - val_precision_58: 0.6380\n",
      "Epoch 78/100\n",
      "926/926 [==============================] - 0s 127us/step - loss: 0.2738 - precision_58: 0.6388 - val_loss: 0.3562 - val_precision_58: 0.6389\n",
      "Epoch 79/100\n",
      "926/926 [==============================] - 0s 128us/step - loss: 0.2810 - precision_58: 0.6393 - val_loss: 0.3610 - val_precision_58: 0.6399\n",
      "Epoch 80/100\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.2755 - precision_58: 0.6397 - val_loss: 0.3676 - val_precision_58: 0.6405\n",
      "Epoch 81/100\n",
      "926/926 [==============================] - 0s 148us/step - loss: 0.2695 - precision_58: 0.6410 - val_loss: 0.3724 - val_precision_58: 0.6412\n",
      "Epoch 82/100\n",
      "926/926 [==============================] - 0s 141us/step - loss: 0.2534 - precision_58: 0.6419 - val_loss: 0.3823 - val_precision_58: 0.6426\n",
      "Epoch 83/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2513 - precision_58: 0.6427 - val_loss: 0.3903 - val_precision_58: 0.6435\n",
      "Epoch 84/100\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.2540 - precision_58: 0.6443 - val_loss: 0.3929 - val_precision_58: 0.6449\n",
      "Epoch 85/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2583 - precision_58: 0.6453 - val_loss: 0.3581 - val_precision_58: 0.6459\n",
      "Epoch 86/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2775 - precision_58: 0.6463 - val_loss: 0.3526 - val_precision_58: 0.6468\n",
      "Epoch 87/100\n",
      "926/926 [==============================] - 0s 129us/step - loss: 0.2610 - precision_58: 0.6472 - val_loss: 0.3578 - val_precision_58: 0.6479\n",
      "Epoch 88/100\n",
      "926/926 [==============================] - 0s 133us/step - loss: 0.2627 - precision_58: 0.6485 - val_loss: 0.3611 - val_precision_58: 0.6488\n",
      "Epoch 89/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2667 - precision_58: 0.6493 - val_loss: 0.3719 - val_precision_58: 0.6498\n",
      "Epoch 90/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2762 - precision_58: 0.6497 - val_loss: 0.3661 - val_precision_58: 0.6501\n",
      "Epoch 91/100\n",
      "926/926 [==============================] - 0s 135us/step - loss: 0.2653 - precision_58: 0.6505 - val_loss: 0.3557 - val_precision_58: 0.6507\n",
      "Epoch 92/100\n",
      "926/926 [==============================] - 0s 134us/step - loss: 0.2797 - precision_58: 0.6509 - val_loss: 0.3587 - val_precision_58: 0.6514\n",
      "Epoch 93/100\n",
      "926/926 [==============================] - 0s 136us/step - loss: 0.2525 - precision_58: 0.6521 - val_loss: 0.3806 - val_precision_58: 0.6529\n",
      "Epoch 94/100\n",
      "926/926 [==============================] - 0s 149us/step - loss: 0.2569 - precision_58: 0.6530 - val_loss: 0.3814 - val_precision_58: 0.6537\n",
      "Epoch 95/100\n",
      "926/926 [==============================] - 0s 151us/step - loss: 0.2587 - precision_58: 0.6540 - val_loss: 0.3913 - val_precision_58: 0.6540\n",
      "Epoch 96/100\n",
      "926/926 [==============================] - 0s 147us/step - loss: 0.2691 - precision_58: 0.6545 - val_loss: 0.3959 - val_precision_58: 0.6550\n",
      "Epoch 97/100\n",
      "926/926 [==============================] - 0s 145us/step - loss: 0.2683 - precision_58: 0.6556 - val_loss: 0.3817 - val_precision_58: 0.6559\n",
      "Epoch 98/100\n",
      "926/926 [==============================] - 0s 140us/step - loss: 0.2592 - precision_58: 0.6563 - val_loss: 0.3851 - val_precision_58: 0.6565\n",
      "Epoch 99/100\n",
      "926/926 [==============================] - 0s 138us/step - loss: 0.2570 - precision_58: 0.6569 - val_loss: 0.3889 - val_precision_58: 0.6573\n",
      "Epoch 100/100\n",
      "926/926 [==============================] - 0s 154us/step - loss: 0.2176 - precision_58: 0.6582 - val_loss: 0.4021 - val_precision_58: 0.6585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8736451c70>"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = 0.3\n",
    "modelc = keras.models.Sequential()\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(100, kernel_initializer= 'he_normal'))\n",
    "modelc.add(keras.layers.BatchNormalization())\n",
    "modelc.add(keras.layers.Activation('relu'))\n",
    "modelc.add(Dropout(dropout))\n",
    "modelc.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modelc.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modelc.fit(X_train, y_train, epochs = 100, batch_size = 20, validation_split=0.1, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96       863\n",
      "           1       0.92      0.57      0.70       166\n",
      "\n",
      "    accuracy                           0.92      1029\n",
      "   macro avg       0.92      0.78      0.83      1029\n",
      "weighted avg       0.92      0.92      0.91      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       370\n",
      "           1       0.69      0.38      0.49        71\n",
      "\n",
      "    accuracy                           0.87       441\n",
      "   macro avg       0.79      0.67      0.71       441\n",
      "weighted avg       0.86      0.87      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modelc.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modelc.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 0s 428us/step - loss: 0.9381 - precision_102: 0.1355 - val_loss: 0.6537 - val_precision_102: 0.1601\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 79us/step - loss: 0.5285 - precision_102: 0.1626 - val_loss: 0.5223 - val_precision_102: 0.1675\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 68us/step - loss: 0.4440 - precision_102: 0.1718 - val_loss: 0.4795 - val_precision_102: 0.1780\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 66us/step - loss: 0.4103 - precision_102: 0.1886 - val_loss: 0.4620 - val_precision_102: 0.1997\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3971 - precision_102: 0.2108 - val_loss: 0.4315 - val_precision_102: 0.2208\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.3806 - precision_102: 0.2305 - val_loss: 0.4217 - val_precision_102: 0.2439\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.3629 - precision_102: 0.2550 - val_loss: 0.4125 - val_precision_102: 0.2672\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.3522 - precision_102: 0.2816 - val_loss: 0.3948 - val_precision_102: 0.2968\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3465 - precision_102: 0.3079 - val_loss: 0.3857 - val_precision_102: 0.3207\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3505 - precision_102: 0.3346 - val_loss: 0.3796 - val_precision_102: 0.3425\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.3501 - precision_102: 0.3532 - val_loss: 0.3681 - val_precision_102: 0.3650\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.3320 - precision_102: 0.3741 - val_loss: 0.3624 - val_precision_102: 0.3823\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3259 - precision_102: 0.3926 - val_loss: 0.3576 - val_precision_102: 0.4017\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 67us/step - loss: 0.3369 - precision_102: 0.4107 - val_loss: 0.3564 - val_precision_102: 0.4190\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 69us/step - loss: 0.3308 - precision_102: 0.4268 - val_loss: 0.3523 - val_precision_102: 0.4327\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 61us/step - loss: 0.3139 - precision_102: 0.4429 - val_loss: 0.3521 - val_precision_102: 0.4507\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.3131 - precision_102: 0.4555 - val_loss: 0.3580 - val_precision_102: 0.4610\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 58us/step - loss: 0.3169 - precision_102: 0.4659 - val_loss: 0.3531 - val_precision_102: 0.4710\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 63us/step - loss: 0.3123 - precision_102: 0.4765 - val_loss: 0.3532 - val_precision_102: 0.4815\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 61us/step - loss: 0.2995 - precision_102: 0.4869 - val_loss: 0.3483 - val_precision_102: 0.4931\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 76us/step - loss: 0.3305 - precision_102: 0.4965 - val_loss: 0.3476 - val_precision_102: 0.5016\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 71us/step - loss: 0.3004 - precision_102: 0.5078 - val_loss: 0.3453 - val_precision_102: 0.5124\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 71us/step - loss: 0.3015 - precision_102: 0.5176 - val_loss: 0.3490 - val_precision_102: 0.5208\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 73us/step - loss: 0.3138 - precision_102: 0.5243 - val_loss: 0.3524 - val_precision_102: 0.5271\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.2953 - precision_102: 0.5300 - val_loss: 0.3478 - val_precision_102: 0.5341\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 76us/step - loss: 0.3008 - precision_102: 0.5372 - val_loss: 0.3462 - val_precision_102: 0.5408\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 73us/step - loss: 0.2924 - precision_102: 0.5448 - val_loss: 0.3441 - val_precision_102: 0.5485\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 71us/step - loss: 0.2964 - precision_102: 0.5518 - val_loss: 0.3460 - val_precision_102: 0.5554\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 70us/step - loss: 0.2833 - precision_102: 0.5582 - val_loss: 0.3465 - val_precision_102: 0.5612\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 75us/step - loss: 0.2991 - precision_102: 0.5640 - val_loss: 0.3463 - val_precision_102: 0.5666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f86b4ee3dc0>"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "modeld = keras.models.Sequential()\n",
    "modeld.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modeld.add(keras.layers.BatchNormalization())\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Activation('relu'))\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modeld.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modeld.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       863\n",
      "           1       0.92      0.39      0.55       166\n",
      "\n",
      "    accuracy                           0.90      1029\n",
      "   macro avg       0.91      0.69      0.74      1029\n",
      "weighted avg       0.90      0.90      0.88      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       370\n",
      "           1       0.75      0.34      0.47        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.82      0.66      0.70       441\n",
      "weighted avg       0.86      0.88      0.85       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modeld.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modeld.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 103 samples\n",
      "Epoch 1/30\n",
      "926/926 [==============================] - 0s 434us/step - loss: 0.2851 - precision_111: 0.1581 - val_loss: 0.2109 - val_precision_111: 0.1429\n",
      "Epoch 2/30\n",
      "926/926 [==============================] - 0s 77us/step - loss: 0.1928 - precision_111: 0.1444 - val_loss: 0.1824 - val_precision_111: 0.1441\n",
      "Epoch 3/30\n",
      "926/926 [==============================] - 0s 68us/step - loss: 0.1808 - precision_111: 0.1461 - val_loss: 0.1697 - val_precision_111: 0.1475\n",
      "Epoch 4/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.1644 - precision_111: 0.1492 - val_loss: 0.1605 - val_precision_111: 0.1587\n",
      "Epoch 5/30\n",
      "926/926 [==============================] - 0s 64us/step - loss: 0.1527 - precision_111: 0.1720 - val_loss: 0.1542 - val_precision_111: 0.1955\n",
      "Epoch 6/30\n",
      "926/926 [==============================] - 0s 61us/step - loss: 0.1554 - precision_111: 0.2159 - val_loss: 0.1495 - val_precision_111: 0.2333\n",
      "Epoch 7/30\n",
      "926/926 [==============================] - 0s 65us/step - loss: 0.1474 - precision_111: 0.2460 - val_loss: 0.1465 - val_precision_111: 0.2658\n",
      "Epoch 8/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.1468 - precision_111: 0.2944 - val_loss: 0.1452 - val_precision_111: 0.3258\n",
      "Epoch 9/30\n",
      "926/926 [==============================] - 0s 64us/step - loss: 0.1395 - precision_111: 0.3586 - val_loss: 0.1428 - val_precision_111: 0.3744\n",
      "Epoch 10/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.1405 - precision_111: 0.3876 - val_loss: 0.1413 - val_precision_111: 0.4104\n",
      "Epoch 11/30\n",
      "926/926 [==============================] - 0s 59us/step - loss: 0.1412 - precision_111: 0.4378 - val_loss: 0.1393 - val_precision_111: 0.4564\n",
      "Epoch 12/30\n",
      "926/926 [==============================] - 0s 60us/step - loss: 0.1411 - precision_111: 0.4707 - val_loss: 0.1386 - val_precision_111: 0.4854\n",
      "Epoch 13/30\n",
      "926/926 [==============================] - 0s 69us/step - loss: 0.1336 - precision_111: 0.5021 - val_loss: 0.1364 - val_precision_111: 0.5200\n",
      "Epoch 14/30\n",
      "926/926 [==============================] - 0s 69us/step - loss: 0.1384 - precision_111: 0.5373 - val_loss: 0.1364 - val_precision_111: 0.5508\n",
      "Epoch 15/30\n",
      "926/926 [==============================] - 0s 67us/step - loss: 0.1378 - precision_111: 0.5603 - val_loss: 0.1373 - val_precision_111: 0.5676\n",
      "Epoch 16/30\n",
      "926/926 [==============================] - 0s 78us/step - loss: 0.1307 - precision_111: 0.5812 - val_loss: 0.1379 - val_precision_111: 0.5898\n",
      "Epoch 17/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1282 - precision_111: 0.6060 - val_loss: 0.1363 - val_precision_111: 0.6198\n",
      "Epoch 18/30\n",
      "926/926 [==============================] - 0s 76us/step - loss: 0.1302 - precision_111: 0.6302 - val_loss: 0.1369 - val_precision_111: 0.6367\n",
      "Epoch 19/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1337 - precision_111: 0.6433 - val_loss: 0.1373 - val_precision_111: 0.6541\n",
      "Epoch 20/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1331 - precision_111: 0.6534 - val_loss: 0.1371 - val_precision_111: 0.6572\n",
      "Epoch 21/30\n",
      "926/926 [==============================] - 0s 68us/step - loss: 0.1267 - precision_111: 0.6681 - val_loss: 0.1385 - val_precision_111: 0.6754\n",
      "Epoch 22/30\n",
      "926/926 [==============================] - 0s 73us/step - loss: 0.1313 - precision_111: 0.6825 - val_loss: 0.1375 - val_precision_111: 0.6884\n",
      "Epoch 23/30\n",
      "926/926 [==============================] - 0s 73us/step - loss: 0.1248 - precision_111: 0.6950 - val_loss: 0.1379 - val_precision_111: 0.6984\n",
      "Epoch 24/30\n",
      "926/926 [==============================] - 0s 70us/step - loss: 0.1291 - precision_111: 0.7035 - val_loss: 0.1380 - val_precision_111: 0.7084\n",
      "Epoch 25/30\n",
      "926/926 [==============================] - 0s 69us/step - loss: 0.1261 - precision_111: 0.7135 - val_loss: 0.1390 - val_precision_111: 0.7182\n",
      "Epoch 26/30\n",
      "926/926 [==============================] - 0s 73us/step - loss: 0.1292 - precision_111: 0.7242 - val_loss: 0.1392 - val_precision_111: 0.7284\n",
      "Epoch 27/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1313 - precision_111: 0.7326 - val_loss: 0.1394 - val_precision_111: 0.7344\n",
      "Epoch 28/30\n",
      "926/926 [==============================] - 0s 77us/step - loss: 0.1240 - precision_111: 0.7405 - val_loss: 0.1410 - val_precision_111: 0.7447\n",
      "Epoch 29/30\n",
      "926/926 [==============================] - 0s 74us/step - loss: 0.1304 - precision_111: 0.7493 - val_loss: 0.1406 - val_precision_111: 0.7500\n",
      "Epoch 30/30\n",
      "926/926 [==============================] - 0s 83us/step - loss: 0.1261 - precision_111: 0.7527 - val_loss: 0.1405 - val_precision_111: 0.7551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8630334fa0>"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight= {0:0.7, 1:0.3}\n",
    "dropout = 0.2\n",
    "modeld = keras.models.Sequential()\n",
    "modeld.add(keras.layers.Dense(100, kernel_initializer= 'he_normal',input_shape = (X.shape[1],)))\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.BatchNormalization())\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Activation('relu'))\n",
    "modeld.add(Dropout(dropout))\n",
    "modeld.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "modeld.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Precision()])\n",
    "modeld.fit(X_train, y_train, epochs = 30, batch_size = 20, validation_split=0.1, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94       863\n",
      "           1       0.95      0.37      0.53       166\n",
      "\n",
      "    accuracy                           0.90      1029\n",
      "   macro avg       0.92      0.68      0.74      1029\n",
      "weighted avg       0.90      0.90      0.87      1029\n",
      "\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.94       370\n",
      "           1       0.88      0.32      0.47        71\n",
      "\n",
      "    accuracy                           0.88       441\n",
      "   macro avg       0.88      0.66      0.70       441\n",
      "weighted avg       0.88      0.88      0.86       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(classification_report(y_train, [x >=0.5 for x in modeld.predict(X_train)]))\n",
    "print('test')\n",
    "print(classification_report(y_test, [x >=0.5 for x in modeld.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "###For Deep Learning we are going to be using modeld\n",
    "###For machine Learning use svc\n",
    "###VC for very high precision and acceptable recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Example of how to use Machine Learning Model - will talk about SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mutiplication = list(X_test.iloc[0])*svc.coef_[0]\n",
    "mutiplication_abs = np.abs(mutiplication)\n",
    "indexes = np.where(mutiplication_abs >= 0.3)\n",
    "plt.barh(X_train.columns,mutiplication, color = '#9fadfa', linewidth = 1, edgecolor = 'black')\n",
    "plt.barh(X_train.columns[indexes],mutiplication[indexes], color = '#FFa5a5', linewidth = 1, edgecolor = 'black')\n",
    "plt.legend(['Not Affecting', 'Affecting'])\n",
    "plt.title('Factors Affecting the Attrition Problem for this Employee')\n",
    "plt.savefig('Factors.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
